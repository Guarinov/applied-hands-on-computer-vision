# Applied Hands-On Computer Vision - HPI WiSe 2025

This repository contains the assessment results and shared codebase for the Hands-On-CV Research Seminar at the Hasso Plattner Institute (WiSe 2025-26).

This repository contains the following assignments:

1. **MNIST Dataset Curation Lab - `Assignment-1`**: Uses FiftyOne to audit label noise and implements a robust "I Don't Know" (IDK) classification strategy using LeNet-5 and Dynamic Focal Loss.
2. **CILP Assessment: Multimodal Learning - `Assignment-2`**: End-to-end study of multimodal fusion and cross-modal adaptation using synthetic RGB and LiDAR data for object classification.
3. \<tbd\>

The code in this repository builds upon the NVIDIA Lab Notebooks and in-class scratchpads. AI tools were used exclusively for bug fixing and improving readability, including enhancing class/function documentation, visualization styling, and docstrings, and were not used for full code generation or modeling decisions.

## ğŸ“‚ Repository Structure
The project follows the "src-layout" to ensure shared logic is easily accessible across different assignments while keeping the root directory clean.

```text
Applied-Hands-On-Computer-Vision/
â”œâ”€â”€ .gitignore               # Git ignore file
â”œâ”€â”€ pyproject.toml           # Project metadata and dependencies
â”œâ”€â”€ LICENSE                  # Project license
â”œâ”€â”€ README.md                # Setup and usage instructions
â”œâ”€â”€ src/
â”‚   â””â”€â”€ handsoncv/           # Main package
â”‚       â”œâ”€â”€ __init__.py      # Logic specific to Assignment 2      
â”‚       â”œâ”€â”€ datasets.py        
â”‚       â”œâ”€â”€ models.py
â”‚       â”œâ”€â”€ training.py
â”‚       â”œâ”€â”€ utils.py
â”‚       â””â”€â”€ visualization.py       
â”œâ”€â”€ tests/                   # Unit tests for src modules
â”‚   â”œâ”€â”€ test_datasets.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_training.py
â”‚   â”œâ”€â”€ test_utils.py
â”‚   â””â”€â”€ test_visualization.py
â”œâ”€â”€ Assignment-01/           # Folder for the first assessment
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ image-dataset-curation-with-specialist-models-LeNet5.ipynb     
â””â”€â”€ Assignment-02/           # Folder for the second assessment
    â”œâ”€â”€ data/                # Local folder not synchronized with GitHub; reproducible with scripts/download_data.py
    â”œâ”€â”€ checkpoints/         # Saved model weights
    â”œâ”€â”€ notebooks/
    â”‚   â”œâ”€â”€ 01_dataset_exploration.ipynb  # Task 1
    â”‚   â””â”€â”€ ...                            # Task 2+
    â”œâ”€â”€ results/             # Figures and tables
    â”œâ”€â”€ scripts/             # Data download script
    â”‚   â””â”€â”€ download_data.py
    â””â”€â”€ subset.json          # .json file containing train/val splits, seed and metadata
```

Please refer to the README.md of the individual `Assignment-*` folder for detailed summaries of results, reproducibility instructions, and code organization.

## ğŸ Setup Guide: Micromamba Environment

This repository uses a virtual environment called `handsoncv`, created with **micromamba**, which is designed to be shared across the various assignments in this course. This guide will walk you through installing **micromamba** and setting up the `handsoncv` environment. This environment contains all the dependencies required to:

- Use the functions in the `src` folder.
- Run the analyses and experiments in the `notebooks` folder.

### 1. Install Micromamba

Micromamba is a tiny, fast, and standalone package manager that doesn't require a base Python installation.

#### macOS / Linux
Run the following in your terminal:
```bash
"${SHELL}" <(curl -L micro.mamba.pm/install.sh)
```
Restart your terminal after installation.

#### Windows (PowerShell)

```powershell
Invoke-Expression ((Invoke-WebRequest -Uri https://micro.mamba.pm/install.ps1).Content)
```

### 2. Create \& Activate the Environment 
Install the project in editable mode to get all dependencies listed in pyproject.toml (including PyTorch â‰¥ 2.0, FiftyOne, etc.):
```bash
micromamba create -n handsoncv python=3.11 -c conda-forge
micromamba activate handsoncv
```

Install the project in editable mode to get all dependencies listed in `pyproject.toml` (including PyTorch â‰¥ 2.0, FiftyOne, etc.):
```bash
pip install -e.
```

#### A Note on Assignment-1
The MNIST Dataset Curation Lab (`Assignment-1`) was originally executed using a standalone virtual environment by installing the dependencies listed in `requirements.txt`,
```bash
(Assignment-1) pip install -r requirement.txt
```
The assignment can also be re-run using the shared `handsoncv` environment, which provides all the required dependencies and ensures consistency across assignments.

### 2. Verify the Installation
To ensure PyTorch and other key packages are installed correctly:
```bash
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); import fiftyone as fo; print('FiftyOne installed')"
```

If you are using VS Code or Jupyter Notebooks, ensure you select the handsoncv kernel. Since ipykernel is included in the dependencies, you can register it manually if it doesn't show up:
```bash
python -m ipykernel install --user --name handsoncv --display-name "Python 3.11 (handsoncv)"
```
### 3. Run Tests 
Once the environment is set up, you can run unit tests for the src/handsoncv modules using pytest:
```bash
pytest tests/ -v
```
This will execute all test files in the `tests/` folder and provide verbose output. Each `test_*.py` file should contain tests for the corresponding module in `src/handsoncv`.




{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a309b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "import json \n",
    "import torch\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from handsoncv.datasets import CILPFusionDataset\n",
    "from handsoncv.models import LidarClassifier, CILPModel, CrossModalProjector, RGB2LiDARClassifier\n",
    "from handsoncv.training import train_fusion_cilp_model\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ROOT_PATH = \"~/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/\"\n",
    "MOUNTED_ROOT_PATH = os.path.expanduser(ROOT_PATH)\n",
    "ROOT_DATA = \"~/Documents/repos/BuildingAIAgentsWithMultimodalModels/data/assessment/\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3c6a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to train with 4799 training pairs and 1200 validation pairs.\n"
     ]
    }
   ],
   "source": [
    "# Load split dictionary previouslu created with 01_dataset_exploration.ipynb\n",
    "mapping_file = \"subset_splits.json\"\n",
    "with open(f\"{MOUNTED_ROOT_PATH}/{mapping_file}\", \"r\") as f:\n",
    "    splits = json.load(f)\n",
    "    \n",
    "torch.manual_seed(splits[\"seed\"])\n",
    "\n",
    "# Instantiate Dataset\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])\n",
    "\n",
    "train_ds = CILPFusionDataset(root_dir=ROOT_DATA, sample_ids=splits[\"train\"], transform=img_transforms)\n",
    "val_ds = CILPFusionDataset(root_dir=ROOT_DATA, sample_ids=splits[\"val\"], transform=img_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "print(f\"Ready to train with {len(train_ds)} training pairs and {len(val_ds)} validation pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb784651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 overlapping IDs.\n",
      "Example leaked IDs: []\n",
      "Class prior average in first training batch: 0.5312, and validation batch: 0.5625\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# Sanity Check - Ensure no data leakage between train and val sets\n",
    "###################################################################\n",
    "\n",
    "assert set(train_ds.sample_ids).isdisjoint(set(val_ds.sample_ids)), \"DATA LEAKAGE DETECTED!\"\n",
    "\n",
    "leaked_ids = set(train_ds.sample_ids).intersection(set(val_ds.sample_ids))\n",
    "print(f\"Found {len(leaked_ids)} overlapping IDs.\")\n",
    "print(f\"Example leaked IDs: {list(leaked_ids)[:10]}\")\n",
    "\n",
    "train_labels = next(iter(train_loader))[-1].cpu().numpy()\n",
    "val_labels = next(iter(val_loader))[-1].cpu().numpy()\n",
    "class_prior_train, class_prior_val = train_labels.mean(), val_labels.mean()\n",
    "\n",
    "print(f\"Class prior average in first training batch: {class_prior_train:.4f}, and validation batch: {class_prior_val:.4f}\")\n",
    "\n",
    "if class_prior_train < 0.01 or class_prior_train > 0.99:\n",
    "    raise ValueError(\"The training batch is extremely imbalanced \"\n",
    "        f\"(class prior = {class_prior_train:.4f}). \"\n",
    "        \"It will cause the model to memorize label ordering. \"\n",
    "        \"Please recreate the dataset splits.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ac701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration common to all subsequent steps\n",
    "SUBSET_SIZE = len(train_ds) + len(val_ds) \n",
    "INTERM_FUSION_EMB_DIM = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc1f17",
   "metadata": {},
   "source": [
    "### Step 5.1a: Train the LiDAR-Only Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbcfdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguarino-vanessa-emanuela\u001b[0m (\u001b[33mhandsoncv-research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251227_020250-w9cz1b9x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/w9cz1b9x' target=\"_blank\">5.1a_Lidar_Only</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/w9cz1b9x' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/w9cz1b9x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Val Loss: 0.6590, Acc: 60.73% | Mem: 138.5MB\n",
      "Epoch 1: Val Loss: 0.4981, Acc: 74.83% | Mem: 138.5MB\n",
      "Epoch 2: Val Loss: 0.2713, Acc: 91.47% | Mem: 138.5MB\n",
      "Epoch 3: Val Loss: 0.0881, Acc: 97.21% | Mem: 138.5MB\n",
      "Epoch 4: Val Loss: 0.0293, Acc: 99.75% | Mem: 138.5MB\n",
      "Epoch 5: Val Loss: 0.0271, Acc: 99.32% | Mem: 138.5MB\n",
      "Epoch 6: Val Loss: 0.0157, Acc: 99.83% | Mem: 138.5MB\n",
      "Epoch 7: Val Loss: 0.0073, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 8: Val Loss: 0.0055, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 9: Val Loss: 0.0040, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 10: Val Loss: 0.0030, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 11: Val Loss: 0.0026, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 12: Val Loss: 0.0024, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 13: Val Loss: 0.0023, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 14: Val Loss: 0.0019, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 15: Val Loss: 0.0019, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 16: Val Loss: 0.0018, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 17: Val Loss: 0.0018, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 18: Val Loss: 0.0018, Acc: 100.00% | Mem: 138.5MB\n",
      "Epoch 19: Val Loss: 0.0018, Acc: 100.00% | Mem: 138.5MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆█████████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▆███████▇▇█▄▂▁▁▁▇▇▇▃</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁███████████████████</td></tr><tr><td>train_loss</td><td>█▇▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>100</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>5.11984</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>138.48975</td></tr><tr><td>train_loss</td><td>0.00108</td></tr><tr><td>val_loss</td><td>0.00175</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">5.1a_Lidar_Only</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/w9cz1b9x' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/w9cz1b9x</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment</a><br>Synced 4 W&B file(s), 20 media file(s), 50 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251227_020250-w9cz1b9x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration to fufill logging requirement\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"handsoncv-cilp-assessment\", \n",
    "    name=\"5.1a_Lidar_Only\",\n",
    "    config={\n",
    "        \"architecture\": \"LidarClassifier\",\n",
    "        \"fusion_strategy\": \"single_modality\",\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"subset_size\": SUBSET_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "# Instantiate Classifier on LiDAR images only\n",
    "lidar_model = LidarClassifier(emb_dim_interm=INTERM_FUSION_EMB_DIM).to(\"cuda\")\n",
    "# lidar_model = Classifier(1).to(\"cuda\")\n",
    "\n",
    "optimizer = torch.optim.Adam(lidar_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "metrics_lidar = train_fusion_cilp_model(\n",
    "    lidar_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer=optimizer, \n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    epochs=EPOCHS, \n",
    "    scheduler=scheduler, \n",
    "    task_mode=\"lidar-only\"\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7dcb2f",
   "metadata": {},
   "source": [
    "### Step 5.1b: Contrastive Pretraining (CILP Alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd67c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguarino-vanessa-emanuela\u001b[0m (\u001b[33mhandsoncv-research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251226_172448-pwoy8ial</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/pwoy8ial' target=\"_blank\">5.1b_CILP_Contrastive</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/pwoy8ial' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/pwoy8ial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Data passed to `wandb.Image` should consist of values in the range [0, 255], image data will be normalized to this range, but behavior will be removed in a future version of wandb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Val Loss: 0.8117, Acc: 0.00% | Mem: 378.5MB\n",
      "Epoch 1: Val Loss: 0.6637, Acc: 0.00% | Mem: 378.5MB\n",
      "Epoch 2: Val Loss: 0.5211, Acc: 0.00% | Mem: 378.5MB\n",
      "Epoch 3: Val Loss: 0.4564, Acc: 0.00% | Mem: 378.5MB\n",
      "Epoch 4: Val Loss: 0.4244, Acc: 0.00% | Mem: 378.5MB\n",
      "✅ Success! CILP Val Loss 0.4244 is below 3.2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time_sec</td><td>▁██▆▆</td></tr><tr><td>learning_rate</td><td>█▇▅▃▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁████</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>epoch_time_sec</td><td>6.37821</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>peak_gpu_mem_mb</td><td>378.52686</td></tr><tr><td>train_loss</td><td>0.38895</td></tr><tr><td>val_loss</td><td>0.42439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">5.1b_CILP_Contrastive</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/pwoy8ial' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment/runs/pwoy8ial</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-cilp-assessment</a><br>Synced 4 W&B file(s), 10 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251226_172448-pwoy8ial/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration to fufill logging requirement\n",
    "EPOCHS = 5 # Based on Nvidia 05_Assessment on MultiModal AI Agents\n",
    "LEARNING_RATE = 1e-4 \n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"handsoncv-cilp-assessment\", \n",
    "    name=\"5.1b_CILP_Contrastive\",\n",
    "    config={\n",
    "        \"architecture\": \"CILPModel\",\n",
    "        \"fusion_strategy\": \"contrastive\",\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"subset_size\": SUBSET_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "cilp_model = CILPModel(emb_dim_interm=INTERM_FUSION_EMB_DIM, emb_dim_late=INTERM_FUSION_EMB_DIM).to(\"cuda\")\n",
    "\n",
    "optimizer = torch.optim.Adam(cilp_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "metrics_cilp = train_fusion_cilp_model(\n",
    "    cilp_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer=optimizer, \n",
    "    criterion=torch.nn.CrossEntropyLoss(), # CrossEntropy is used for CLIP loss\n",
    "    device=\"cuda\", \n",
    "    epochs=EPOCHS, \n",
    "    scheduler=scheduler, \n",
    "    task_mode=\"contrastive\"\n",
    ")\n",
    "\n",
    "# CHECK REQUIREMENT:\n",
    "if metrics_cilp['val_loss'] < 3.2:\n",
    "    print(f\"✅ Success! CILP Val Loss {metrics_cilp['val_loss']:.4f} is below 3.2\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f4f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsoncv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f59234",
   "metadata": {},
   "source": [
    "## Multi-Modal Embedder Fusion Architecture Comparison\n",
    "\n",
    "In this notebook, we extend the analysis started with data visualization and preparation to conduct multimodal machine learning experiments focused on fusion strategies for combining multiple data modalities. Different modalities capture complementary information which, when fused, can lead to more robust and performant models.\n",
    "\n",
    "We focus on two fusion strategies, whose architectural choices, advantages, and limitations are discussed in detail in the `Experiments` section:\n",
    "\n",
    "- **Late Fusion**, where modalities are processed independently and combined at a later stage;  \n",
    "- **Intermediate Fusion**, where intermediate representations from unimodal networks are combined via concatenation, addition, or multiplication.  \n",
    "\n",
    "In our setting, the two modalities are RGB and LiDAR images, previously analyzed in Notebook 01. The task is binary classification of object shape (cube vs. sphere). We implement both fusion strategies and, for intermediate fusion, its three variants, performing a structured design exploration to identify the best-performing architecture for this task and modality pair.\n",
    "\n",
    "The first section of this notebook defines the experimental configuration, including path setup and initialization of the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d454f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using GPU: NVIDIA GeForce RTX 3090\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "import json \n",
    "import torch\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from handsoncv.datasets import CILPFusionDataset\n",
    "from handsoncv.models import LateFusionNet, IntermediateFusionNet\n",
    "from handsoncv.training import train_fusion_cilp_model\n",
    "from handsoncv.utils import set_seed, seed_worker\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\", \"..\"))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# Folders we frequently use across the experiments' notebooks\n",
    "ROOT_PATH = os.path.join(PROJECT_ROOT, \"Assignment-2\")\n",
    "RESULTS_DIR = os.path.join(ROOT_PATH, \"results\", \"tables\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True) # Ensures folder exists \n",
    "\n",
    "CHECKPOINTS_DIR = os.path.join(ROOT_PATH, \"checkpoints\")\n",
    "ROOT_DATA = \"~/Documents/repos/BuildingAIAgentsWithMultimodalModels/data/assessment/\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94ae5a",
   "metadata": {},
   "source": [
    "In the following cell, we set a fixed random seed to ensure reproducible data shuffling in the DataLoader multiprocessing pipeline. We then use a custom data-loading function implemented in `src/datasets.py`, which constructs the training and validation splits from predefined sample lists. These lists were generated and saved earlier in `01_dataset_exploration.ipynb`. For details on the creation procedure, refer to `01_dataset_exploration.ipynb`, and for information on the subset size used in the experiments, see the configuration logs in the public [handsoncv-fusion project link](https://wandb.ai/handsoncv-research/handsoncv-fusion?nw=nwuserguarinovanessaemanuela)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c2d19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility.\n",
      "Ready to train with 4799 training pairs and 1200 validation pairs.\n"
     ]
    }
   ],
   "source": [
    "# Load split dictionary previouslu created with 01_dataset_exploration.ipynb\n",
    "mapping_file = \"subset_splits.json\"\n",
    "with open(f\"{ROOT_PATH}/{mapping_file}\", \"r\") as f:\n",
    "    splits = json.load(f)\n",
    "    \n",
    "SEED = splits[\"seed\"] # From .json file created through notebook 01_dataset_exploration.ipynb \n",
    "set_seed(SEED)\n",
    "\n",
    "# Instantiate Dataset and relative Transformation\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])\n",
    "\n",
    "train_ds = CILPFusionDataset(root_dir=ROOT_DATA, sample_ids=splits[\"train\"], transform=img_transforms)\n",
    "val_ds = CILPFusionDataset(root_dir=ROOT_DATA, sample_ids=splits[\"val\"], transform=img_transforms)\n",
    "\n",
    "# Create a Generator object to pass to the dataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "print(f\"Ready to train with {len(train_ds)} training pairs and {len(val_ds)} validation pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561a987",
   "metadata": {},
   "source": [
    "Finally, the last configuration cell ensures a balanced distribution of classes within the training and validation batches. This is particularly important because **the datasets provided in the NVIDIA notebooks produced batches containing only a single class**, leading to unreliable accuracy estimates. These three configuration cells are shared across the experimental notebooks `02_*`, `03_*`, and `04_*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f5c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 overlapping IDs.\n",
      "Example leaked IDs: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prior average in first training batch: 0.2812, and validation batch: 0.5625\n"
     ]
    }
   ],
   "source": [
    "assert set(train_ds.sample_ids).isdisjoint(set(val_ds.sample_ids)), \"DATA LEAKAGE DETECTED!\"\n",
    "\n",
    "leaked_ids = set(train_ds.sample_ids).intersection(set(val_ds.sample_ids))\n",
    "print(f\"Found {len(leaked_ids)} overlapping IDs.\")\n",
    "print(f\"Example leaked IDs: {list(leaked_ids)[:10]}\")\n",
    "\n",
    "train_labels = next(iter(train_loader))[-1].cpu().numpy()\n",
    "val_labels = next(iter(val_loader))[-1].cpu().numpy()\n",
    "class_prior_train, class_prior_val = train_labels.mean(), val_labels.mean()\n",
    "\n",
    "print(f\"Class prior average in first training batch: {class_prior_train:.4f}, and validation batch: {class_prior_val:.4f}\")\n",
    "\n",
    "if class_prior_train < 0.01 or class_prior_train > 0.99:\n",
    "    raise ValueError(\"The training batch is extremely imbalanced \"\n",
    "        f\"(class prior = {class_prior_train:.4f}). \"\n",
    "        \"It will cause the model to memorize label ordering. \"\n",
    "        \"Please recreate the dataset splits.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8ac45",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9b5f7",
   "metadata": {},
   "source": [
    "### Model Architectures and Fusion Strategies\n",
    "\n",
    "This section describes the neural architectures evaluated in the experiments, beginning with the shared embedding backbone and followed by the multimodal fusion strategies considered.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Embedding Backbone**\n",
    "\n",
    "All models and also the cross-modal projection stratgey we will implement in the notebook `04_final_assessment.ipynb` employ a common `Embedder` architecture, adapted from the NVIDIA assessment baseline and designed to process each modality independently. The embedder consists of a sequence of convolutional layers with ReLU activations and progressive spatial downsampling. The channel progression follows a compact design:\n",
    "$\n",
    "\\text{in\\_channels} \\rightarrow 50 \\rightarrow 100 \\rightarrow 200 \\rightarrow 200\n",
    "$\n",
    ".Downsampling is performed either via max pooling or strided convolution, depending on the experimental configuration, whose ablation is provided in the notebook `03_strided_conv_ablation.ipynb`.\n",
    "\n",
    "For intermediate fusion, the `Embedder` outputs a spatial feature map of size \\([B, 200, 4, 4]\\), preserving intermediate spatial structure for cross-modal interactions. For late fusion, an additional projection head maps the flattened feature map to a low-dimensional embedding vector of dimension $\\texttt{emb\\_dim\\_late}$, enabling fusion at an almost final representation level. $\\texttt{emb\\_dim\\_late}$ is set to be 2 in the following experiments, to concatenate fetaures almost mapped to their final rerpesnetation.\n",
    "\n",
    "2. **Late Fusion Architecture**\n",
    "\n",
    "In the Late Fusion model, RGB and LiDAR inputs are processed independently by two embedders configured to output compact embedding vectors of size $[B, \\texttt{emb\\_dim\\_late}]$. These modality-specific embeddings are then easily concatenated and passed to a shared classifier. This approach enforces modality separation and enables independent feature learning for each input stream.\n",
    "\n",
    "Late fusion is expected to be robust to modality-specific noise, to be modular and avoid higehr dimensional issues that cross-modal intercation and concatenation at intermeidat estages would cause. However, because cross-modal interactions occur only at the final stage, the model may be limited in its ability to capture fine-grained spatial correspondences between modalities.\n",
    "\n",
    "3. **Intermediate Fusion Architectures**\n",
    "\n",
    "In **Intermediate Fusion**, RGB and LiDAR inputs are encoded into spatial feature maps of size $[B, 200, 4, 4]$ and fused prior to classification, enabling the network to learn joint representations with explicit spatial alignment across modalities. Three fusion operators are evaluated:\n",
    "\n",
    "- *Element-wise Addition*\n",
    "Feature maps are combined through element-wise addition, producing a tensor of size $[B, 200, 4, 4]$. This his operation enforces shared semantics across and is parameter-efficient and computationally inexpensive. However, it may limit expressiveness when the modalities encode complementary rather than redundant information.\n",
    "\n",
    "- *Hadamard Product*\n",
    "Element-wise multiplication combines the feature maps into a tensor of size $[B, 200, 4, 4]$, emphasizing features that are simultaneously salient in both modalities. This acts as an implicit local attention mechanism but may suppress informative features when one modality is weak, potentially affecting gradient propagation and optimization stability.\n",
    "\n",
    "- *Channel-wise Concatenation*\n",
    "Feature maps are concatenated along the channel dimension, yielding a fused representation of size $[B, 400, 4, 4]$. This strategy preserves all modality-specific information and provides the highest representational capacity, at the cost of increased parameter count, memory usage, and optimization complexity.\n",
    "\n",
    "---\n",
    "\n",
    "> *Expected Trade-offs.*  \n",
    "> Late fusion prioritizes modularity and robustness, whereas intermediate fusion enables earlier cross-modal interactions at the cost of increased computational and optimization complexity. Moreover, identifying the representation level at which modalities can be meaningfully aligned is not straightforward. Among intermediate fusion strategies, addition and multiplication introduce stronger inductive biases and improved efficiency, but the latter carries a risk of overfitting, while concatenation maximizes expressiveness with higher complexity and an increased risk of overfitting. The subsequent experiments quantitatively evaluate these trade-offs in terms of performance, convergence stability, and memory efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cbce4e",
   "metadata": {},
   "source": [
    "In the following, we perform the proposed suite of experiments using the `dynamic_train_fusion_cilp_model` function (implemented in `src/training.py`), logging parameters and curves at the following public [handsoncv-fusion project link](https://wandb.ai/handsoncv-research/handsoncv-fusion?nw=nwuserguarinovanessaemanuela). Please refer to the latest runs as the main runs; previous ones are left to illustrate experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e18d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguarino-vanessa-emanuela\u001b[0m (\u001b[33mhandsoncv-research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251226_163231-i5q3961i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/i5q3961i' target=\"_blank\">Late Fusion</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/i5q3961i' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/i5q3961i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Late Fusion...\n",
      "Epoch 0: Val Loss: 0.5511, Acc: 70.27% | Mem: 377.8MB\n",
      "Epoch 1: Val Loss: 0.4688, Acc: 77.53% | Mem: 377.8MB\n",
      "Epoch 2: Val Loss: 0.3927, Acc: 81.50% | Mem: 377.8MB\n",
      "Epoch 3: Val Loss: 0.3010, Acc: 86.82% | Mem: 377.8MB\n",
      "Epoch 4: Val Loss: 0.2231, Acc: 92.23% | Mem: 377.8MB\n",
      "Epoch 5: Val Loss: 0.1434, Acc: 95.02% | Mem: 377.8MB\n",
      "Epoch 6: Val Loss: 0.0861, Acc: 96.79% | Mem: 377.8MB\n",
      "Epoch 7: Val Loss: 0.0553, Acc: 97.97% | Mem: 377.8MB\n",
      "Epoch 8: Val Loss: 0.0387, Acc: 98.56% | Mem: 377.8MB\n",
      "Epoch 9: Val Loss: 0.0285, Acc: 99.16% | Mem: 377.8MB\n",
      "Epoch 10: Val Loss: 0.0263, Acc: 99.32% | Mem: 377.8MB\n",
      "Epoch 11: Val Loss: 0.0366, Acc: 98.56% | Mem: 377.8MB\n",
      "Epoch 12: Val Loss: 0.0217, Acc: 99.24% | Mem: 377.8MB\n",
      "Epoch 13: Val Loss: 0.0111, Acc: 99.83% | Mem: 377.8MB\n",
      "Epoch 14: Val Loss: 0.0171, Acc: 99.32% | Mem: 377.8MB\n",
      "Epoch 15: Val Loss: 0.0114, Acc: 99.66% | Mem: 377.8MB\n",
      "Epoch 16: Val Loss: 0.0098, Acc: 99.83% | Mem: 377.8MB\n",
      "Epoch 17: Val Loss: 0.0102, Acc: 99.75% | Mem: 377.8MB\n",
      "Epoch 18: Val Loss: 0.0111, Acc: 99.58% | Mem: 377.8MB\n",
      "Epoch 19: Val Loss: 0.0109, Acc: 99.58% | Mem: 377.8MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▄▅▆▇▇█████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▆▂▂▂▂▂▁▂▁▂▂▃▂▂▂▃▂▅▃█</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁███████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>99.5777</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>6.39039</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>377.8252</td></tr><tr><td>train_loss</td><td>0.00424</td></tr><tr><td>val_loss</td><td>0.01089</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Late Fusion</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/i5q3961i' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/i5q3961i</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a><br>Synced 4 W&B file(s), 20 media file(s), 50 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251226_163231-i5q3961i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251226_163431-qc1jz18t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qc1jz18t' target=\"_blank\">Int Fusion Concat</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qc1jz18t' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qc1jz18t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Int Fusion Concat...\n",
      "Epoch 0: Val Loss: 0.5065, Acc: 74.58% | Mem: 505.3MB\n",
      "Epoch 1: Val Loss: 0.3979, Acc: 81.42% | Mem: 505.3MB\n",
      "Epoch 2: Val Loss: 0.1699, Acc: 93.33% | Mem: 505.3MB\n",
      "Epoch 3: Val Loss: 0.0391, Acc: 98.65% | Mem: 505.3MB\n",
      "Epoch 4: Val Loss: 0.0256, Acc: 98.90% | Mem: 505.3MB\n",
      "Epoch 5: Val Loss: 0.0104, Acc: 99.75% | Mem: 505.3MB\n",
      "Epoch 6: Val Loss: 0.0111, Acc: 99.66% | Mem: 505.3MB\n",
      "Epoch 7: Val Loss: 0.0042, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 8: Val Loss: 0.0035, Acc: 99.92% | Mem: 505.3MB\n",
      "Epoch 9: Val Loss: 0.0032, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 10: Val Loss: 0.0040, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 11: Val Loss: 0.0036, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 12: Val Loss: 0.0035, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 13: Val Loss: 0.0032, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 14: Val Loss: 0.0038, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 15: Val Loss: 0.0038, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 16: Val Loss: 0.0036, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 17: Val Loss: 0.0033, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 18: Val Loss: 0.0035, Acc: 99.83% | Mem: 505.3MB\n",
      "Epoch 19: Val Loss: 0.0034, Acc: 99.83% | Mem: 505.3MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▆█████████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▂▃▂▂▂▂▂▂▂▁▁▁▂▁▁▂▁▂▁█</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁███████████████████</td></tr><tr><td>train_loss</td><td>█▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>99.83108</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>6.17162</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>505.28174</td></tr><tr><td>train_loss</td><td>0.00023</td></tr><tr><td>val_loss</td><td>0.00341</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Int Fusion Concat</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qc1jz18t' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qc1jz18t</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a><br>Synced 4 W&B file(s), 20 media file(s), 50 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251226_163431-qc1jz18t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251226_163628-p6uzbs3q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/p6uzbs3q' target=\"_blank\">Int Fusion Add</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/p6uzbs3q' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/p6uzbs3q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Int Fusion Add...\n",
      "Epoch 0: Val Loss: 0.5319, Acc: 75.93% | Mem: 510.4MB\n",
      "Epoch 1: Val Loss: 0.4354, Acc: 78.63% | Mem: 510.4MB\n",
      "Epoch 2: Val Loss: 0.3588, Acc: 84.46% | Mem: 510.4MB\n",
      "Epoch 3: Val Loss: 0.0451, Acc: 99.16% | Mem: 510.4MB\n",
      "Epoch 4: Val Loss: 0.0108, Acc: 99.92% | Mem: 510.4MB\n",
      "Epoch 5: Val Loss: 0.0063, Acc: 99.92% | Mem: 510.4MB\n",
      "Epoch 6: Val Loss: 0.0022, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 7: Val Loss: 0.0015, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 8: Val Loss: 0.0015, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 9: Val Loss: 0.0016, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 10: Val Loss: 0.0012, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 11: Val Loss: 0.0012, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 12: Val Loss: 0.0010, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 13: Val Loss: 0.0007, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 14: Val Loss: 0.0007, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 15: Val Loss: 0.0007, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 16: Val Loss: 0.0007, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 17: Val Loss: 0.0007, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 18: Val Loss: 0.0007, Acc: 100.00% | Mem: 510.4MB\n",
      "Epoch 19: Val Loss: 0.0007, Acc: 100.00% | Mem: 510.4MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▂▃█████████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>█▇█▅▁▁▃▇▄▁▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁███████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>100</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>5.29866</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>510.44971</td></tr><tr><td>train_loss</td><td>0.00017</td></tr><tr><td>val_loss</td><td>0.00067</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Int Fusion Add</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/p6uzbs3q' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/p6uzbs3q</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a><br>Synced 4 W&B file(s), 20 media file(s), 50 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251226_163628-p6uzbs3q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251226_163825-vusbewfn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/vusbewfn' target=\"_blank\">Int Fusion Mul</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/vusbewfn' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/vusbewfn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Int Fusion Mul...\n",
      "Epoch 0: Val Loss: 0.4381, Acc: 79.31% | Mem: 565.7MB\n",
      "Epoch 1: Val Loss: 0.1454, Acc: 93.75% | Mem: 565.7MB\n",
      "Epoch 2: Val Loss: 0.0431, Acc: 98.82% | Mem: 565.7MB\n",
      "Epoch 3: Val Loss: 0.0538, Acc: 98.56% | Mem: 565.7MB\n",
      "Epoch 4: Val Loss: 0.0295, Acc: 99.49% | Mem: 565.7MB\n",
      "Epoch 5: Val Loss: 0.0227, Acc: 99.49% | Mem: 565.7MB\n",
      "Epoch 6: Val Loss: 0.0151, Acc: 99.58% | Mem: 565.7MB\n",
      "Epoch 7: Val Loss: 0.0121, Acc: 99.75% | Mem: 565.7MB\n",
      "Epoch 8: Val Loss: 0.2296, Acc: 94.43% | Mem: 565.7MB\n",
      "Epoch 9: Val Loss: 0.0397, Acc: 99.16% | Mem: 565.7MB\n",
      "Epoch 10: Val Loss: 0.0398, Acc: 99.16% | Mem: 565.7MB\n",
      "Epoch 11: Val Loss: 0.0262, Acc: 99.32% | Mem: 565.7MB\n",
      "Epoch 12: Val Loss: 0.0223, Acc: 99.58% | Mem: 565.7MB\n",
      "Epoch 13: Val Loss: 0.0203, Acc: 99.58% | Mem: 565.7MB\n",
      "Epoch 14: Val Loss: 0.0239, Acc: 99.49% | Mem: 565.7MB\n",
      "Epoch 15: Val Loss: 0.0237, Acc: 99.49% | Mem: 565.7MB\n",
      "Epoch 16: Val Loss: 0.0241, Acc: 99.49% | Mem: 565.7MB\n",
      "Epoch 17: Val Loss: 0.0241, Acc: 99.49% | Mem: 565.7MB\n",
      "Epoch 18: Val Loss: 0.0242, Acc: 99.49% | Mem: 565.7MB\n",
      "Epoch 19: Val Loss: 0.0244, Acc: 99.49% | Mem: 565.7MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▆██████▆███████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▇▂▃▇▇▇▅▁▁▂▁▁▂▁▁▁▁▂▁█</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁███████████████████</td></tr><tr><td>train_loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>99.49324</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>6.11958</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>565.68018</td></tr><tr><td>train_loss</td><td>0.00022</td></tr><tr><td>val_loss</td><td>0.02441</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Int Fusion Mul</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/vusbewfn' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/vusbewfn</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a><br>Synced 4 W&B file(s), 20 media file(s), 50 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251226_163825-vusbewfn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL FUSION COMPARISON TABLE\n",
      "============================================================\n",
      "     Architecture  val_loss   accuracy   params  sec_per_epoch  gpu_mem_mb\n",
      "      Late Fusion  0.010889  99.577703 13694510       5.610209  377.825195\n",
      "Int Fusion Concat  0.003413  99.831081 13627934       5.460804  505.281738\n",
      "   Int Fusion Add  0.000675 100.000000  7074334       5.503140  510.449707\n",
      "   Int Fusion Mul  0.024415  99.493243  7074334       5.552202  565.680176\n"
     ]
    }
   ],
   "source": [
    "# Configuration to fufill logging requirement\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "SUBSET_SIZE = len(train_ds) + len(val_ds) \n",
    "LATE_FUSION_EMB_DIM = 2\n",
    "INTERM_FUSION_EMB_DIM = 200\n",
    "\n",
    "# Define Experiment Suite\n",
    "strategies = [\n",
    "    (\"Late Fusion\", LateFusionNet(emb_dim_interm=INTERM_FUSION_EMB_DIM, emb_dim_late=LATE_FUSION_EMB_DIM), \"late\"),\n",
    "    (\"Int Fusion Concat\", IntermediateFusionNet(mode='concat', emb_dim_interm=INTERM_FUSION_EMB_DIM), \"intermediate_concat\"),\n",
    "    (\"Int Fusion Add\", IntermediateFusionNet(mode='add', emb_dim_interm=INTERM_FUSION_EMB_DIM), \"intermediate_add\"),\n",
    "    (\"Int Fusion Mul\", IntermediateFusionNet(mode='mul', emb_dim_interm=INTERM_FUSION_EMB_DIM), \"intermediate_mul\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model, strategy_type in strategies:\n",
    "    current_emb_size = LATE_FUSION_EMB_DIM if strategy_type == \"late\" else INTERM_FUSION_EMB_DIM\n",
    "    run = wandb.init(\n",
    "        project=\"handsoncv-fusion\", \n",
    "        name=name,\n",
    "        config={\n",
    "            \"architecture\": name,\n",
    "            \"fusion_strategy\": strategy_type,\n",
    "            \"embedding_size\": current_emb_size,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"optimizer_type\": \"Adam\",\n",
    "            \"subset_size\": SUBSET_SIZE,\n",
    "            \"seed\": splits[\"seed\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS) #T_max set to the total number of epochs\n",
    "    \n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    metrics = train_fusion_cilp_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        optimizer=optimizer,\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        epochs=EPOCHS,\n",
    "        scheduler=scheduler,\n",
    "        task_mode=\"fusion\"\n",
    "    )\n",
    "    \n",
    "    metrics['Architecture'] = name\n",
    "    results.append(metrics)\n",
    "    wandb.finish()\n",
    "\n",
    "# --- Final Comparison Table (Task 3.4) ---\n",
    "# Create DataFrame and reorder columns to match assignment table\n",
    "df = pd.DataFrame(results)\n",
    "cols = [\"Architecture\", \"val_loss\", \"accuracy\", \"params\", \"sec_per_epoch\", \"gpu_mem_mb\"]\n",
    "comparison_table = df[cols]\n",
    "\n",
    "# Display the table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL FUSION COMPARISON TABLE\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd6985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsoncv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

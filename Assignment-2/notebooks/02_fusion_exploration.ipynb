{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f59234",
   "metadata": {},
   "source": [
    "## Multi-Modal Embedder Fusion Architecture Comparison\n",
    "\n",
    "In this notebook, we extend the analysis started with data visualization and preparation to conduct multimodal machine learning experiments focused on fusion strategies for combining multiple data modalities. Different modalities capture complementary information which, when fused, can lead to more robust and performant models.\n",
    "\n",
    "We focus on two fusion strategies, whose architectural choices, advantages, and limitations are discussed in detail in the `Experiments` section:\n",
    "\n",
    "- **Late Fusion**, where modalities are processed independently and combined at a later stage;  \n",
    "- **Intermediate Fusion**, where intermediate representations from unimodal networks are combined via concatenation, addition, or multiplication.  \n",
    "\n",
    "In our setting, the two modalities are RGB and LiDAR images, previously analyzed in Notebook 01. The task is binary classification of object shape (cube vs. sphere). We implement both fusion strategies and, for intermediate fusion, its three variants, performing a structured design exploration to identify the best-performing architecture for this task and modality pair.\n",
    "\n",
    "The first section of this notebook defines the experimental configuration, including path setup and initialization of the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d454f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3090\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "import json \n",
    "import torch\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from handsoncv.datasets import CILPFusionDataset\n",
    "from handsoncv.models import LateFusionNet, IntermediateFusionNet\n",
    "from handsoncv.training import train_fusion_cilp_model\n",
    "from handsoncv.utils import set_seed, seed_worker\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\", \"..\"))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# Folders we frequently use across the experiments' notebooks\n",
    "ROOT_PATH = os.path.join(PROJECT_ROOT, \"Assignment-2\")\n",
    "RESULTS_DIR = os.path.join(ROOT_PATH, \"results\", \"tables\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True) # Ensures folder exists \n",
    "\n",
    "CHECKPOINTS_DIR = os.path.join(ROOT_PATH, \"checkpoints\")\n",
    "ROOT_DATA = \"~/Documents/repos/BuildingAIAgentsWithMultimodalModels/data/assessment/\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94ae5a",
   "metadata": {},
   "source": [
    "In the following cell, we set a fixed random seed to ensure reproducible data shuffling in the DataLoader multiprocessing pipeline. We then use a custom data-loading function implemented in `src/datasets.py`, which constructs the training and validation splits from predefined sample lists. These lists were generated and saved earlier in `01_dataset_exploration.ipynb`. For details on the creation procedure, refer to `01_dataset_exploration.ipynb`, and for information on the subset size used in the experiments, see the configuration logs in the public [handsoncv-fusion project link](https://wandb.ai/handsoncv-research/handsoncv-fusion?nw=nwuserguarinovanessaemanuela)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c2d19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility.\n",
      "Ready to train with 4799 training pairs and 1200 validation pairs.\n"
     ]
    }
   ],
   "source": [
    "# Load split dictionary previouslu created with 01_dataset_exploration.ipynb\n",
    "mapping_file = \"subset_splits.json\"\n",
    "with open(f\"{ROOT_PATH}/{mapping_file}\", \"r\") as f:\n",
    "    splits = json.load(f)\n",
    "    \n",
    "SEED = splits[\"seed\"] # From .json file created through notebook 01_dataset_exploration.ipynb \n",
    "set_seed(SEED)\n",
    "\n",
    "# Instantiate Dataset and relative Transformation\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])\n",
    "\n",
    "train_ds = CILPFusionDataset(root_dir=ROOT_DATA, sample_ids=splits[\"train\"], transform=img_transforms)\n",
    "val_ds = CILPFusionDataset(root_dir=ROOT_DATA, sample_ids=splits[\"val\"], transform=img_transforms)\n",
    "\n",
    "# Create a Generator object to pass to the dataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "print(f\"Ready to train with {len(train_ds)} training pairs and {len(val_ds)} validation pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561a987",
   "metadata": {},
   "source": [
    "Finally, the last configuration cell ensures a balanced distribution of classes within the training and validation batches. This is particularly important because **the datasets provided in the NVIDIA notebooks produced batches containing only a single class**, leading to unreliable accuracy estimates. These three configuration cells are shared across the experimental notebooks `02_*`, `03_*`, and `04_*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f5c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 overlapping IDs.\n",
      "Example leaked IDs: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prior average in first training batch: 0.2812, and validation batch: 0.5625\n"
     ]
    }
   ],
   "source": [
    "assert set(train_ds.sample_ids).isdisjoint(set(val_ds.sample_ids)), \"DATA LEAKAGE DETECTED!\"\n",
    "\n",
    "leaked_ids = set(train_ds.sample_ids).intersection(set(val_ds.sample_ids))\n",
    "print(f\"Found {len(leaked_ids)} overlapping IDs.\")\n",
    "print(f\"Example leaked IDs: {list(leaked_ids)[:10]}\")\n",
    "\n",
    "train_labels = next(iter(train_loader))[-1].cpu().numpy()\n",
    "val_labels = next(iter(val_loader))[-1].cpu().numpy()\n",
    "class_prior_train, class_prior_val = train_labels.mean(), val_labels.mean()\n",
    "\n",
    "print(f\"Class prior average in first training batch: {class_prior_train:.4f}, and validation batch: {class_prior_val:.4f}\")\n",
    "\n",
    "if class_prior_train < 0.01 or class_prior_train > 0.99:\n",
    "    raise ValueError(\"The training batch is extremely imbalanced \"\n",
    "        f\"(class prior = {class_prior_train:.4f}). \"\n",
    "        \"It will cause the model to memorize label ordering. \"\n",
    "        \"Please recreate the dataset splits.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8ac45",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9b5f7",
   "metadata": {},
   "source": [
    "### Model Architectures and Fusion Strategies\n",
    "\n",
    "This section describes the neural architectures evaluated in the experiments, beginning with the shared embedding backbone and followed by the multimodal fusion strategies considered.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Embedding Backbone**\n",
    "\n",
    "All models and also the cross-modal projection stratgey we will implement in the notebook `04_final_assessment.ipynb` employ a common `Embedder` architecture, adapted from the NVIDIA assessment baseline and designed to process each modality independently. The embedder consists of a sequence of convolutional layers with ReLU activations and progressive spatial downsampling. The channel progression follows a compact design:\n",
    "$\n",
    "\\text{in\\_channels} \\rightarrow 50 \\rightarrow 100 \\rightarrow 200 \\rightarrow 200\n",
    "$\n",
    ".Downsampling is performed either via max pooling or strided convolution, depending on the experimental configuration, whose ablation is provided in the notebook `03_strided_conv_ablation.ipynb`.\n",
    "\n",
    "For intermediate fusion, the `Embedder` outputs a spatial feature map of size \\([B, 200, 4, 4]\\), preserving intermediate spatial structure for cross-modal interactions. For late fusion, an additional projection head maps the flattened feature map to a low-dimensional embedding vector of dimension $\\texttt{emb\\_dim\\_late}$, enabling fusion at an almost final representation level. $\\texttt{emb\\_dim\\_late}$ is set to be 2 in the following experiments, to concatenate fetaures almost mapped to their final rerpesnetation.\n",
    "\n",
    "2. **Late Fusion Architecture**\n",
    "\n",
    "In the Late Fusion model, RGB and LiDAR inputs are processed independently by two embedders configured to output compact embedding vectors of size $[B, \\texttt{emb\\_dim\\_late}]$. These modality-specific embeddings are then easily concatenated and passed to a shared classifier. This approach enforces modality separation and enables independent feature learning for each input stream.\n",
    "\n",
    "Late fusion is expected to be robust to modality-specific noise, to be modular and avoid higher dimensional issues that cross-modal intercation and concatenation at intermediate stages would cause. However, because cross-modal interactions occur only at the final stage, the model may be limited in its ability to capture fine-grained spatial correspondences between modalities.\n",
    "\n",
    "3. **Intermediate Fusion Architectures**\n",
    "\n",
    "In **Intermediate Fusion**, RGB and LiDAR inputs are encoded into spatial feature maps of size $[B, 200, 4, 4]$ and fused prior to classification, enabling the network to learn joint representations with explicit spatial alignment across modalities. Three fusion operators are evaluated:\n",
    "\n",
    "- *Element-wise Addition*\n",
    "Feature maps are combined through element-wise addition, producing a tensor of size $[B, 200, 4, 4]$. This his operation enforces shared semantics across and is parameter-efficient and computationally inexpensive. However, it may limit expressiveness when the modalities encode complementary rather than redundant information.\n",
    "\n",
    "- *Hadamard Product*\n",
    "Element-wise multiplication combines the feature maps into a tensor of size $[B, 200, 4, 4]$, emphasizing features that are simultaneously salient in both modalities. This acts as an implicit local attention mechanism but may suppress informative features when one modality is weak, potentially affecting gradient propagation and optimization stability.\n",
    "\n",
    "- *Channel-wise Concatenation*\n",
    "Feature maps are concatenated along the channel dimension, yielding a fused representation of size $[B, 400, 4, 4]$. This strategy preserves all modality-specific information and provides the highest representational capacity, at the cost of increased parameter count, memory usage, and optimization complexity.\n",
    "\n",
    "---\n",
    "\n",
    "> *Expected Trade-offs.*  \n",
    "> Late fusion prioritizes modularity and robustness, whereas intermediate fusion enables earlier cross-modal interactions at the cost of increased computational and optimization complexity. Moreover, identifying the representation level at which modalities can be meaningfully aligned is not straightforward. Among intermediate fusion strategies, addition and multiplication introduce stronger inductive biases and improved efficiency, but the latter carries a risk of overfitting, while concatenation maximizes expressiveness with higher complexity and an increased risk of overfitting. The subsequent experiments quantitatively evaluate these trade-offs in terms of performance, convergence stability, and memory efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cbce4e",
   "metadata": {},
   "source": [
    "In the following, we perform the proposed suite of experiments using the `dynamic_train_fusion_cilp_model` function (implemented in `src/training.py`), logging parameters and curves at the following public [handsoncv-fusion project link](https://wandb.ai/handsoncv-research/handsoncv-fusion?nw=nwuserguarinovanessaemanuela). Please refer to the latest runs as the main runs; previous ones are left to illustrate experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e18d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguarino-vanessa-emanuela\u001b[0m (\u001b[33mhandsoncv-research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251229_205723-k526zyxk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/k526zyxk' target=\"_blank\">Late Fusion</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/k526zyxk' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/k526zyxk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Late Fusion...\n",
      "{'architecture': 'Late Fusion', 'fusion_strategy': 'late', 'downsample_mode': 'maxpool', 'embedding_size': 2, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 20, 'optimizer_type': 'Adam', 'subset_size': 5999, 'seed': 42, 'number_of_parameters': 1994793}\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 0: Val Loss: 0.4879, Acc: 75.84% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 1: Val Loss: 0.4216, Acc: 80.32% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 2: Val Loss: 0.3844, Acc: 83.87% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 3: Val Loss: 0.2746, Acc: 89.78% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 4: Val Loss: 0.1775, Acc: 93.58% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 5: Val Loss: 0.1284, Acc: 94.59% | Mem: 218.9MB\n",
      "Epoch 6: Val Loss: 0.1429, Acc: 94.00% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 7: Val Loss: 0.0703, Acc: 96.96% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 8: Val Loss: 0.0674, Acc: 96.62% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 9: Val Loss: 0.0557, Acc: 97.13% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 10: Val Loss: 0.0463, Acc: 97.30% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 11: Val Loss: 0.0368, Acc: 98.06% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 12: Val Loss: 0.0306, Acc: 98.23% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 13: Val Loss: 0.0288, Acc: 98.23% | Mem: 218.9MB\n",
      "Epoch 14: Val Loss: 0.0295, Acc: 97.97% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 15: Val Loss: 0.0249, Acc: 98.48% | Mem: 218.9MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_late_maxpool_best_model.pt\n",
      "Epoch 16: Val Loss: 0.0245, Acc: 98.82% | Mem: 218.9MB\n",
      "Epoch 17: Val Loss: 0.0251, Acc: 99.16% | Mem: 218.9MB\n",
      "Epoch 18: Val Loss: 0.0254, Acc: 99.16% | Mem: 218.9MB\n",
      "Epoch 19: Val Loss: 0.0262, Acc: 99.16% | Mem: 218.9MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▂▃▅▆▇▆▇▇▇▇█████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>█▂▁▂▂▂▂▂▁▂▂▃▂▃▂▃▂▁▂▇</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁███████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▅▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>99.15541</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>2.99262</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>218.86475</td></tr><tr><td>train_loss</td><td>0.01401</td></tr><tr><td>val_loss</td><td>0.02617</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Late Fusion</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/k526zyxk' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/k526zyxk</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a><br>Synced 4 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_205723-k526zyxk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251229_205825-xzutamly</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/xzutamly' target=\"_blank\">Int Fusion Concat</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/xzutamly' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/xzutamly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Int Fusion Concat...\n",
      "{'architecture': 'Int Fusion Concat', 'fusion_strategy': 'intermediate_concat', 'downsample_mode': 'maxpool', 'embedding_size': 200, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 20, 'optimizer_type': 'Adam', 'subset_size': 5999, 'seed': 42, 'number_of_parameters': 4517805}\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 0: Val Loss: 0.4735, Acc: 77.62% | Mem: 276.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 1: Val Loss: 0.4348, Acc: 78.04% | Mem: 276.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 2: Val Loss: 0.0808, Acc: 98.56% | Mem: 276.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 3: Val Loss: 0.0413, Acc: 98.31% | Mem: 276.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 4: Val Loss: 0.0065, Acc: 99.92% | Mem: 276.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 5: Val Loss: 0.0062, Acc: 99.75% | Mem: 276.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 6: Val Loss: 0.0031, Acc: 99.92% | Mem: 276.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 7: Val Loss: 0.0028, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 8: Val Loss: 0.0036, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 9: Val Loss: 0.0028, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 10: Val Loss: 0.0028, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 11: Val Loss: 0.0032, Acc: 99.92% | Mem: 276.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_concat_maxpool_best_model.pt\n",
      "Epoch 12: Val Loss: 0.0027, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 13: Val Loss: 0.0032, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 14: Val Loss: 0.0030, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 15: Val Loss: 0.0031, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 16: Val Loss: 0.0028, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 17: Val Loss: 0.0029, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 18: Val Loss: 0.0029, Acc: 99.92% | Mem: 276.5MB\n",
      "Epoch 19: Val Loss: 0.0029, Acc: 99.92% | Mem: 276.5MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁█▇████████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▃█▂▃▃▂▃▄▃▃▂▁▄▃▂▄▂▂▃▃</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁███████████████████</td></tr><tr><td>train_loss</td><td>█▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>99.91554</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>2.7394</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>276.52637</td></tr><tr><td>train_loss</td><td>7e-05</td></tr><tr><td>val_loss</td><td>0.00291</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Int Fusion Concat</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/xzutamly' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/xzutamly</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a><br>Synced 4 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_205825-xzutamly/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251229_205926-9pqjbix9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/9pqjbix9' target=\"_blank\">Int Fusion Add</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/9pqjbix9' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/9pqjbix9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Int Fusion Add...\n",
      "{'architecture': 'Int Fusion Add', 'fusion_strategy': 'intermediate_add', 'downsample_mode': 'maxpool', 'embedding_size': 200, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 20, 'optimizer_type': 'Adam', 'subset_size': 5999, 'seed': 42, 'number_of_parameters': 2879405}\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_add_maxpool_best_model.pt\n",
      "Epoch 0: Val Loss: 0.4819, Acc: 76.60% | Mem: 287.8MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_add_maxpool_best_model.pt\n",
      "Epoch 1: Val Loss: 0.4305, Acc: 79.39% | Mem: 287.8MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_add_maxpool_best_model.pt\n",
      "Epoch 2: Val Loss: 0.3316, Acc: 84.97% | Mem: 287.8MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_add_maxpool_best_model.pt\n",
      "Epoch 3: Val Loss: 0.0866, Acc: 97.30% | Mem: 287.8MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_add_maxpool_best_model.pt\n",
      "Epoch 4: Val Loss: 0.0080, Acc: 99.75% | Mem: 287.8MB\n",
      "Epoch 5: Val Loss: 0.0625, Acc: 97.47% | Mem: 287.8MB\n",
      "Epoch 6: Val Loss: 0.0125, Acc: 99.49% | Mem: 287.8MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_add_maxpool_best_model.pt\n",
      "Epoch 7: Val Loss: 0.0024, Acc: 99.92% | Mem: 287.8MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_add_maxpool_best_model.pt\n",
      "Epoch 8: Val Loss: 0.0016, Acc: 100.00% | Mem: 287.8MB\n",
      "Epoch 9: Val Loss: 0.0021, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 10: Val Loss: 0.0017, Acc: 99.92% | Mem: 287.8MB\n",
      "Epoch 11: Val Loss: 0.0019, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 12: Val Loss: 0.0019, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 13: Val Loss: 0.0017, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 14: Val Loss: 0.0019, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 15: Val Loss: 0.0019, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 16: Val Loss: 0.0020, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 17: Val Loss: 0.0021, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 18: Val Loss: 0.0021, Acc: 99.83% | Mem: 287.8MB\n",
      "Epoch 19: Val Loss: 0.0021, Acc: 99.83% | Mem: 287.8MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▂▄▇█▇██████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>█▂▂▂▃▁█▂▃▃▇▂▁▁▁▁▂▂▂▂</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>99.83108</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>2.73674</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>287.75098</td></tr><tr><td>train_loss</td><td>9e-05</td></tr><tr><td>val_loss</td><td>0.00209</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Int Fusion Add</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/9pqjbix9' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/9pqjbix9</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a><br>Synced 4 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_205926-9pqjbix9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251229_210027-qe2oos7h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qe2oos7h' target=\"_blank\">Int Fusion Mul</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qe2oos7h' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qe2oos7h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Int Fusion Mul...\n",
      "{'architecture': 'Int Fusion Mul', 'fusion_strategy': 'intermediate_mul', 'downsample_mode': 'maxpool', 'embedding_size': 200, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 20, 'optimizer_type': 'Adam', 'subset_size': 5999, 'seed': 42, 'number_of_parameters': 2879405}\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_mul_maxpool_best_model.pt\n",
      "Epoch 0: Val Loss: 0.4196, Acc: 80.49% | Mem: 311.7MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_mul_maxpool_best_model.pt\n",
      "Epoch 1: Val Loss: 0.1887, Acc: 93.16% | Mem: 311.7MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_mul_maxpool_best_model.pt\n",
      "Epoch 2: Val Loss: 0.1190, Acc: 96.20% | Mem: 311.7MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_mul_maxpool_best_model.pt\n",
      "Epoch 3: Val Loss: 0.0313, Acc: 98.90% | Mem: 311.7MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_mul_maxpool_best_model.pt\n",
      "Epoch 4: Val Loss: 0.0293, Acc: 98.99% | Mem: 311.7MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_mul_maxpool_best_model.pt\n",
      "Epoch 5: Val Loss: 0.0286, Acc: 99.16% | Mem: 311.7MB\n",
      "Epoch 6: Val Loss: 0.0536, Acc: 98.23% | Mem: 311.7MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_intermediate_mul_maxpool_best_model.pt\n",
      "Epoch 7: Val Loss: 0.0086, Acc: 99.58% | Mem: 311.7MB\n",
      "Epoch 8: Val Loss: 0.0109, Acc: 99.66% | Mem: 311.7MB\n",
      "Epoch 9: Val Loss: 0.0195, Acc: 99.49% | Mem: 311.7MB\n",
      "Epoch 10: Val Loss: 0.0102, Acc: 99.66% | Mem: 311.7MB\n",
      "Epoch 11: Val Loss: 0.0148, Acc: 99.75% | Mem: 311.7MB\n",
      "Epoch 12: Val Loss: 0.0142, Acc: 99.75% | Mem: 311.7MB\n",
      "Epoch 13: Val Loss: 0.0136, Acc: 99.75% | Mem: 311.7MB\n",
      "Epoch 14: Val Loss: 0.0139, Acc: 99.75% | Mem: 311.7MB\n",
      "Epoch 15: Val Loss: 0.0141, Acc: 99.75% | Mem: 311.7MB\n",
      "Epoch 16: Val Loss: 0.0141, Acc: 99.75% | Mem: 311.7MB\n",
      "Epoch 17: Val Loss: 0.0142, Acc: 99.75% | Mem: 311.7MB\n",
      "Epoch 18: Val Loss: 0.0142, Acc: 99.75% | Mem: 311.7MB\n",
      "Epoch 19: Val Loss: 0.0142, Acc: 99.75% | Mem: 311.7MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▆▇███▇█████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▂▃▂▂▄▃▃▃▃█▁▁▂▂▂▆▃▁▂▆</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁███████████████████</td></tr><tr><td>train_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>99.74662</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time_sec</td><td>2.89828</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>311.7334</td></tr><tr><td>train_loss</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.01422</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Int Fusion Mul</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qe2oos7h' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion/runs/qe2oos7h</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-fusion' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-fusion</a><br>Synced 4 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_210027-qe2oos7h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL FUSION COMPARISON TABLE\n",
      "============================================================\n",
      "     Architecture  val_loss  accuracy  params  sec_per_epoch  gpu_mem_mb\n",
      "      Late Fusion  0.026174 99.155405 1994793       2.777429  218.864746\n",
      "Int Fusion Concat  0.002907 99.915541 4517805       2.747253  276.526367\n",
      "   Int Fusion Add  0.002091 99.831081 2879405       2.765888  287.750977\n",
      "   Int Fusion Mul  0.014217 99.746622 2879405       2.768414  311.733398\n"
     ]
    }
   ],
   "source": [
    "# Configuration to fufill logging requirement\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "SUBSET_SIZE = len(train_ds) + len(val_ds) \n",
    "LATE_FUSION_EMB_DIM = 2\n",
    "INTERM_FUSION_EMB_DIM = 200\n",
    "\n",
    "# Ensure reproducibility\n",
    "SEED = splits[\"seed\"]\n",
    "set_seed(SEED)\n",
    "\n",
    "# Define Experiment Suite\n",
    "strategies = [\n",
    "    (\"Late Fusion\", LateFusionNet(emb_dim_interm=INTERM_FUSION_EMB_DIM, num_classes=1, emb_dim_late=LATE_FUSION_EMB_DIM), \"late\"),\n",
    "    (\"Int Fusion Concat\", IntermediateFusionNet(mode='concat', num_classes=1, emb_dim_interm=INTERM_FUSION_EMB_DIM), \"intermediate_concat\"),\n",
    "    (\"Int Fusion Add\", IntermediateFusionNet(mode='add', num_classes=1, emb_dim_interm=INTERM_FUSION_EMB_DIM), \"intermediate_add\"),\n",
    "    (\"Int Fusion Mul\", IntermediateFusionNet(mode='mul', num_classes=1, emb_dim_interm=INTERM_FUSION_EMB_DIM), \"intermediate_mul\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model, strategy_type in strategies:\n",
    "    current_emb_size = LATE_FUSION_EMB_DIM if strategy_type == \"late\" else INTERM_FUSION_EMB_DIM\n",
    "    run = wandb.init(\n",
    "        project=\"handsoncv-fusion\", \n",
    "        name=name,\n",
    "        config={\n",
    "            \"architecture\": name,\n",
    "            \"fusion_strategy\": strategy_type,\n",
    "            \"downsample_mode\": \"maxpool\",\n",
    "            \"embedding_size\": current_emb_size,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"optimizer_type\": \"Adam\",\n",
    "            \"subset_size\": SUBSET_SIZE,\n",
    "            \"seed\": splits[\"seed\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS) #T_max set to the total number of epochs\n",
    "    \n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    metrics = train_fusion_cilp_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        optimizer=optimizer,\n",
    "        criterion=torch.nn.BCEWithLogitsLoss(),\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        epochs=EPOCHS,\n",
    "        scheduler=scheduler,\n",
    "        task_mode=\"fusion\"\n",
    "    )\n",
    "    \n",
    "    metrics['Architecture'] = name\n",
    "    results.append(metrics)\n",
    "    wandb.finish()\n",
    "\n",
    "# --- Final Comparison Table (Task 3.4) ---\n",
    "# Create DataFrame and reorder columns to match assignment table\n",
    "df = pd.DataFrame(results)\n",
    "cols = [\"Architecture\", \"val_loss\", \"accuracy\", \"params\", \"sec_per_epoch\", \"gpu_mem_mb\"]\n",
    "comparison_table = df[cols]\n",
    "\n",
    "# Display the table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL FUSION COMPARISON TABLE\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ecae6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>params</th>\n",
       "      <th>sec_per_epoch</th>\n",
       "      <th>gpu_mem_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Late Fusion</td>\n",
       "      <td>0.026174</td>\n",
       "      <td>99.155405</td>\n",
       "      <td>1994793</td>\n",
       "      <td>2.777429</td>\n",
       "      <td>218.864746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Int Fusion Concat</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>99.915541</td>\n",
       "      <td>4517805</td>\n",
       "      <td>2.747253</td>\n",
       "      <td>276.526367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Int Fusion Add</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>99.831081</td>\n",
       "      <td>2879405</td>\n",
       "      <td>2.765888</td>\n",
       "      <td>287.750977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Int Fusion Mul</td>\n",
       "      <td>0.014217</td>\n",
       "      <td>99.746622</td>\n",
       "      <td>2879405</td>\n",
       "      <td>2.768414</td>\n",
       "      <td>311.733398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Architecture  val_loss   accuracy   params  sec_per_epoch  gpu_mem_mb\n",
       "0        Late Fusion  0.026174  99.155405  1994793       2.777429  218.864746\n",
       "1  Int Fusion Concat  0.002907  99.915541  4517805       2.747253  276.526367\n",
       "2     Int Fusion Add  0.002091  99.831081  2879405       2.765888  287.750977\n",
       "3     Int Fusion Mul  0.014217  99.746622  2879405       2.768414  311.733398"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d6d2c",
   "metadata": {},
   "source": [
    "## Results Analysis \n",
    "\n",
    "The experimental results reveal a **clear performance gap between Intermediate and Late Fusion strategies**. All three intermediate variants achieved near-perfect validation accuracy (`>99.7%`) and substantially lower validation losses (`0.002–0.014`) compared to the Late Fusion baseline (`99.15%` accuracy, `0.026` loss). Notably, Intermediate Addition achieved the lowest validation loss (`0.0021`) with only `2.8M parameters`, while Intermediate Concatenation reached the highest accuracy (`99.91%`) at the cost of a larger model (`4.5M parameters`). Training logs show that intermediate strategies converge rapidly, often stabilizing within the first five epochs, whereas Late Fusion exhibits slower and more erratic learning. These results highlight the expected trade-off discussed at the start fo this section: Late Fusion simplifies feature combination, while Intermediate Fusion better exploits cross-modal correlations for improved performance\n",
    "\n",
    "The superiority of intermediate fusion can be attributed to the spatially registered synthetic data. RGB and LiDAR modalities produce highly correlated features: for instance, a cube’s sharp vertex in the RGB image aligns with a depth discontinuity in the LiDAR data. Late Fusion’s higher validation loss suggests a potential \"feature collapse,\" where the model over-relies on RGB features and compresses the LiDAR signal into a narrow embedding, limiting cross-modal alignment of geometrically meaningful features.\n",
    "\n",
    "Among intermediate strategies, the choice depends on priorities:\n",
    "- *Memory efficiency and generalization:* Intermediate Addition offers low validation loss, efficient parameter usage, and rapid, confident convergence.  \n",
    "- *Maximum performance:* Intermediate Concatenation achieves the highest accuracy at the cost of increased parameters and complexity.\n",
    "\n",
    "In summary, intermediate-level fusion is essential to preserve cross-modal spatial synergy for geometric classification. Element-wise addition provides the most efficient path to high-confidence convergence, while concatenation offers the richest representation with maximal accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec6c64",
   "metadata": {},
   "source": [
    "> **Important Caveat.**  \n",
    "> These conclusions apply only to the current setup. Data augmentations and regularization methods (e.g., dropout) were not used and could affect the relative performance of the variants. The same applies to Batch Normalization, which might improve Late Fusion and enhance generalization to real-world data. For classification tasks with noisy inputs, Late Fusion could potentially outperform Intermediate Fusion, particularly when it is not straightforward to determine the most meaningful layer at which to concatenate features.\n",
    "\n",
    "> **Extra Observation.**  \n",
    "> Previous experiments (available at [handsoncv-fusion project link](https://wandb.ai/handsoncv-research/handsoncv-fusion?nw=nwuserguarinovanessaemanuela)) show that training the binary task with `CrossEntropyLoss()` and a 2D target slightly improves convergence for the Late Fusion strategy. This supports our earlier expectation that Late Fusion is more competitive when ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsoncv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d30afb",
   "metadata": {},
   "source": [
    "## Ablation Study of Convolutional Downsampling in the Embedder Net\n",
    "\n",
    "In this notebook, we conduct an ablation study on the convolutional layers that constitute the first sequential block of the `Embedder` class. The objective is to analyze how different downsampling strategies affect foreground classification performance. Specifically, we compare two architectural components used for spatial downsampling:\n",
    "\n",
    "- **Strided Convolution**  \n",
    "  A sequence of `nn.Conv2d(out_c, out_c, kernel_size=3, stride=2, padding=1)` followed by `nn.ReLU()`\n",
    "\n",
    "- **Max Pooling**  \n",
    "  A `nn.MaxPool2d(kernel_size=2)` operation\n",
    "\n",
    "These alternatives are selected via the `downsample_mode` argument in the `Embedder` class, as implemented in `src/models.py`. The comparison is performed both:\n",
    "quantitatively, using metrics such as classification accuracy, validation loss, number of model parameters, and training time, and qualitatively, through theoretical analysis of the representational and optimization implications of each approach.\n",
    "\n",
    "In the first part of this notebook, we define the experiment configuration, including path setup and initialization of the relevant data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaab8017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3090\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "import json \n",
    "import torch\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from handsoncv.datasets import CILPFusionDataset\n",
    "from handsoncv.models import IntermediateFusionNet\n",
    "from handsoncv.training import train_fusion_cilp_model\n",
    "from handsoncv.utils import set_seed, seed_worker\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\", \"..\"))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "ROOT_PATH = os.path.join(PROJECT_ROOT, \"Assignment-2\")\n",
    "CHECKPOINTS_DIR = os.path.join(ROOT_PATH, \"checkpoints\")\n",
    "ROOT_DATA = \"~/Documents/repos/BuildingAIAgentsWithMultimodalModels/data/assessment/\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd09aa2",
   "metadata": {},
   "source": [
    "In the following cell, we set a fixed random seed to ensure reproducible data shuffling in the DataLoader multiprocessing pipeline. We then use a custom data-loading function, implemented in `src/datasets.py` which takes a predefined list of samples to construct the training and validation splits. These sample lists were generated and saved earlier using the notebook `01_dataset_exploration.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccec52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to train with 4799 training pairs and 1200 validation pairs.\n"
     ]
    }
   ],
   "source": [
    "# Load split dictionary previouslu created with 01_dataset_exploration.ipynb\n",
    "mapping_file = \"subset_splits.json\"\n",
    "with open(f\"{ROOT_PATH}/{mapping_file}\", \"r\") as f:\n",
    "    splits = json.load(f)\n",
    "    \n",
    "SEED = splits[\"seed\"] # From .json file created through notebook 01_dataset_exploration.ipynb \n",
    "set_seed(SEED)\n",
    "\n",
    "# Instantiate Dataset and relative Transformation\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])\n",
    "\n",
    "train_ds = CILPFusionDataset(root_dir=ROOT_DATA, sample_ids=splits[\"train\"], transform=img_transforms)\n",
    "val_ds = CILPFusionDataset(root_dir=ROOT_DATA, sample_ids=splits[\"val\"], transform=img_transforms)\n",
    "\n",
    "# Create a Generator object to pass to the dataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "print(f\"Ready to train with {len(train_ds)} training pairs and {len(val_ds)} validation pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e13cd5",
   "metadata": {},
   "source": [
    "Finally, the last configuration cell ensures a balanced distribution of classes within the training and validation batches. This is particularly important because **the datasets provided in the NVIDIA notebooks produced batches containing only a single class**, leading to unreliable accuracy estimates. These three configuration cells are shared across the experimental notebooks `02_*`, `03_*`, and `04_*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23380f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 overlapping IDs.\n",
      "Example leaked IDs: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prior average in first training batch: 0.2812, and validation batch: 0.5625\n"
     ]
    }
   ],
   "source": [
    "assert set(train_ds.sample_ids).isdisjoint(set(val_ds.sample_ids)), \"DATA LEAKAGE DETECTED!\"\n",
    "\n",
    "leaked_ids = set(train_ds.sample_ids).intersection(set(val_ds.sample_ids))\n",
    "print(f\"Found {len(leaked_ids)} overlapping IDs.\")\n",
    "print(f\"Example leaked IDs: {list(leaked_ids)[:10]}\")\n",
    "\n",
    "train_labels = next(iter(train_loader))[-1].cpu().numpy()\n",
    "val_labels = next(iter(val_loader))[-1].cpu().numpy()\n",
    "class_prior_train, class_prior_val = train_labels.mean(), val_labels.mean()\n",
    "\n",
    "print(f\"Class prior average in first training batch: {class_prior_train:.4f}, and validation batch: {class_prior_val:.4f}\")\n",
    "\n",
    "if class_prior_train < 0.01 or class_prior_train > 0.99:\n",
    "    raise ValueError(\"The training batch is extremely imbalanced \"\n",
    "        f\"(class prior = {class_prior_train:.4f}). \"\n",
    "        \"It will cause the model to memorize label ordering. \"\n",
    "        \"Please recreate the dataset splits.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9950c",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "In the following cell, we perform the proposed suite of experiments using the `dynamic_train_fusion_cilp_model` function (implemented in `src/training.py`), logging parameters and curves at the following public [handoncv-maxpoolvsstride project link](https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride?nw=nwuserguarinovanessaemanuela). Please refer to the latest runs as the main runs; previous ones are left to illustrate experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c589252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguarino-vanessa-emanuela\u001b[0m (\u001b[33mhandsoncv-research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251229_114027-lz2d5wn9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/lz2d5wn9' target=\"_blank\">MaxPool2d (Baseline)</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/lz2d5wn9' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/lz2d5wn9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Experiment: MaxPool2d (Baseline)\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 0: Val Loss: 0.4864, Acc: 77.11% | Mem: 234.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 1: Val Loss: 0.2766, Acc: 87.84% | Mem: 234.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 2: Val Loss: 0.0380, Acc: 98.90% | Mem: 234.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 3: Val Loss: 0.0134, Acc: 99.49% | Mem: 234.5MB\n",
      "Epoch 4: Val Loss: 0.0260, Acc: 99.07% | Mem: 234.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 5: Val Loss: 0.0049, Acc: 99.83% | Mem: 234.5MB\n",
      "Epoch 6: Val Loss: 0.0050, Acc: 99.83% | Mem: 234.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 7: Val Loss: 0.0018, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 8: Val Loss: 0.0020, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 9: Val Loss: 0.0025, Acc: 99.92% | Mem: 234.5MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 10: Val Loss: 0.0016, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 11: Val Loss: 0.0023, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 12: Val Loss: 0.0023, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 13: Val Loss: 0.0021, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 14: Val Loss: 0.0028, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 15: Val Loss: 0.0025, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 16: Val Loss: 0.0020, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 17: Val Loss: 0.0019, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 18: Val Loss: 0.0023, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 19: Val Loss: 0.0021, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 20: Val Loss: 0.0020, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 21: Val Loss: 0.0021, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 22: Val Loss: 0.0020, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 23: Val Loss: 0.0021, Acc: 99.92% | Mem: 234.5MB\n",
      "Epoch 24: Val Loss: 0.0020, Acc: 99.92% | Mem: 234.5MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄███████████████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>█▅▄▃▃▄▃▂▃▃▂█▂▃▇▂▃▃▃▃▃▁▂▂▂</td></tr><tr><td>learning_rate</td><td>█████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>99.91554</td></tr><tr><td>epoch</td><td>24</td></tr><tr><td>epoch_time_sec</td><td>2.84143</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>234.52344</td></tr><tr><td>train_loss</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.00204</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MaxPool2d (Baseline)</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/lz2d5wn9' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/lz2d5wn9</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride</a><br>Synced 4 W&B file(s), 25 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_114027-lz2d5wn9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/notebooks/wandb/run-20251229_114147-ponnr28j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/ponnr28j' target=\"_blank\">Strided Conv (Ablation)</a></strong> to <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/ponnr28j' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/ponnr28j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Experiment: Strided Conv (Ablation)\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 0: Val Loss: 0.4745, Acc: 77.03% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 1: Val Loss: 0.3749, Acc: 81.67% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 2: Val Loss: 0.2967, Acc: 86.57% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 3: Val Loss: 0.1993, Acc: 93.16% | Mem: 262.0MB\n",
      "Epoch 4: Val Loss: 0.2111, Acc: 90.88% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 5: Val Loss: 0.1173, Acc: 96.45% | Mem: 262.0MB\n",
      "Epoch 6: Val Loss: 0.1533, Acc: 95.52% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 7: Val Loss: 0.1023, Acc: 96.71% | Mem: 262.0MB\n",
      "Epoch 8: Val Loss: 0.1157, Acc: 96.03% | Mem: 262.0MB\n",
      "Epoch 9: Val Loss: 0.1139, Acc: 96.79% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 10: Val Loss: 0.0848, Acc: 97.55% | Mem: 262.0MB\n",
      "Epoch 11: Val Loss: 0.1166, Acc: 96.62% | Mem: 262.0MB\n",
      "Epoch 12: Val Loss: 0.1114, Acc: 96.62% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 13: Val Loss: 0.0812, Acc: 97.64% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 14: Val Loss: 0.0682, Acc: 97.80% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 15: Val Loss: 0.0620, Acc: 98.06% | Mem: 262.0MB\n",
      "Epoch 16: Val Loss: 0.0679, Acc: 97.80% | Mem: 262.0MB\n",
      "Epoch 17: Val Loss: 0.0677, Acc: 97.72% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 18: Val Loss: 0.0521, Acc: 98.31% | Mem: 262.0MB\n",
      "Epoch 19: Val Loss: 0.0606, Acc: 97.89% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 20: Val Loss: 0.0475, Acc: 98.65% | Mem: 262.0MB\n",
      "Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-2/checkpoints/fusion_best_model.pt\n",
      "Epoch 21: Val Loss: 0.0468, Acc: 98.56% | Mem: 262.0MB\n",
      "Epoch 22: Val Loss: 0.0509, Acc: 98.48% | Mem: 262.0MB\n",
      "Epoch 23: Val Loss: 0.0474, Acc: 98.65% | Mem: 262.0MB\n",
      "Epoch 24: Val Loss: 0.0477, Acc: 98.73% | Mem: 262.0MB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▂▄▆▅▇▇▇▇▇█▇▇████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▄▂▃▄▂▃▂▃▇▁▃▂▃▃▃▃▄▁▁▁▁█▂▂▃</td></tr><tr><td>learning_rate</td><td>█████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▃▄▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>98.73311</td></tr><tr><td>epoch</td><td>24</td></tr><tr><td>epoch_time_sec</td><td>2.87414</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>peak_gpu_mem_mb</td><td>262.03027</td></tr><tr><td>train_loss</td><td>0.039</td></tr><tr><td>val_loss</td><td>0.04772</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Strided Conv (Ablation)</strong> at: <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/ponnr28j' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride/runs/ponnr28j</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride' target=\"_blank\">https://wandb.ai/handsoncv-research/handsoncv-maxpoolvsstride</a><br>Synced 4 W&B file(s), 25 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_114147-ponnr28j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TASK 4 COMPARISON TABLE\n",
      "==================================================\n",
      "            val_loss   accuracy     params  total_time_sec  sec_per_epoch  \\\n",
      "Variant                                                                     \n",
      "maxpool     0.002042  99.915541  2879405.0       77.538469       2.904426   \n",
      "stride      0.047720  98.733108  4545505.0       76.158781       2.875165   \n",
      "Difference  0.045678  -1.182432  1666100.0       -1.379688      -0.029260   \n",
      "\n",
      "            gpu_mem_mb  \n",
      "Variant                 \n",
      "maxpool     234.523438  \n",
      "stride      262.030273  \n",
      "Difference   27.506836  \n"
     ]
    }
   ],
   "source": [
    "# Configuration to fufill logging requirement\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "SUBSET_SIZE = len(train_ds) + len(val_ds) \n",
    "INTERM_FUSION_EMB_DIM = 200\n",
    "\n",
    "# Ensure reproducibility\n",
    "SEED = splits[\"seed\"]\n",
    "set_seed(SEED)\n",
    "\n",
    "# Define Ablation Suite\n",
    "experiments = [\n",
    "    (\"MaxPool2d (Baseline)\", IntermediateFusionNet(mode='add', num_classes=1, emb_dim_interm=INTERM_FUSION_EMB_DIM, downsample_mode='maxpool'), \"maxpool\"),\n",
    "    (\"Strided Conv (Ablation)\", IntermediateFusionNet(mode='add', num_classes=1, emb_dim_interm=INTERM_FUSION_EMB_DIM, downsample_mode='stride'), \"stride\")\n",
    "]\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for name, model, mode_tag in experiments:\n",
    "    # We log and dynamically onto wandb with the configuration parameters required by Task 1\n",
    "    run = wandb.init(\n",
    "        project=\"handsoncv-maxpoolvsstride\",\n",
    "        name=name,\n",
    "        config={\n",
    "            \"architecture\": \"Int Fusion Add\",\n",
    "            \"downsample_mode\": mode_tag,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"fusion_strategy\": \"intermediate_add\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    print(f\"\\nRunning Experiment: {name}\")\n",
    "    metrics = train_fusion_cilp_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        optimizer=optimizer, \n",
    "        criterion=torch.nn.BCEWithLogitsLoss(), #torch.nn.CrossEntropyLoss(),\n",
    "        device=\"cuda\", \n",
    "        epochs=EPOCHS, \n",
    "        scheduler=scheduler, \n",
    "        task_mode=\"fusion\"\n",
    "    )\n",
    "    \n",
    "    # Store results for the final table\n",
    "    # Store for local summary table\n",
    "    metrics['Variant'] = mode_tag\n",
    "    # metrics['Parameters'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    ablation_results.append(metrics)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# --- Final Comparison Table (Task 4.2) ---\n",
    "# Create DataFrame and reorder columns\n",
    "df_abl = pd.DataFrame(ablation_results).set_index(\"Variant\")\n",
    "# Calculate diff column \n",
    "df_abl.loc['Difference'] = df_abl.loc['stride'] - df_abl.loc['maxpool']\n",
    "\n",
    "# Display the table\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 4 COMPARISON TABLE\")\n",
    "print(\"=\"*50)\n",
    "print(df_abl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03b8a2",
   "metadata": {},
   "source": [
    "## Results Analysis \n",
    "Most state-of-the-art CNNs for object recognition alternate convolutional and pooling layers, followed by a few fully connected layers. Following [(Springenberg et al., 2015)](https://arxiv.org/pdf/1412.6806), let $f$ be a feature map with dimensions width × height × channels. The $p$-norm for downsampling with pooling size $k$ and stride $r$ is a 3D array $s(f)$ defined as:\n",
    "\n",
    "$$\n",
    "s_{i,j,u}(f) = \\left(\\sum_{h=-k/2}^{k/2} \\sum_{w=-k/2}^{k/2} \\left| f_{g(h,w,i,j,u)} \\right|^p \\right)^{1/p}, \\,\\text{with} \\quad\n",
    "g(h,w,i,j,u) = (r \\cdot i + h, r \\cdot j + w, u)\n",
    "$$\n",
    "\n",
    "Setting $p = \\infty$ gives max pooling. Otherwise, applying a convolution layer gives:\n",
    "\n",
    "$$\n",
    "c_{i,j,o}(f) = \\sigma \\left( \\sum_{h=-k/2}^{k/2} \\sum_{w=-k/2}^{k/2} \\sum_{u=1} \\theta_{h,w,u,o} * f_{h,w,i,j,u,o} \\right)\n",
    "$$\n",
    "\n",
    "where $\\theta$ are learnable weights, $\\sigma(\\cdot)$ is a nonlinear activation, and $o$ indexes output channels.  \n",
    "\n",
    "This shows that pooling can be seen as a feature-wise convolution with $\\theta_{h,w,u,o}=1$ and $o=u$, but replacing the activation with the $p$-norm. \n",
    "\n",
    "While strided convolution increases the number of learnable parameters, pooling introduces invariance via the $p$-norm, reduces spatial dimensions to cover larger input regions in higher layers, and simplifies optimization by keeping features separate. However, unlike pooling, strided convolutions have learnable weights, allowing the network to decide which features to preserve or emphasize during downsampling, and gradients propagate through them, potentially capturing more spatial information and feature interactions compared to non-learnable layers. In summary, strided convolutions trade off the simplicity and invariance of pooling for richer, learnable feature extraction and larger effective receptive fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa111887",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsoncv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c20885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Seeds set to 42 for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import wandb\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# On a multi-GPU system, this hides all GPUs except the first \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "# Custom modules\n",
    "from handsoncv.datasets import generate_clip_metadata, TFflowersCLIPDataset\n",
    "from handsoncv.models import UNet \n",
    "from handsoncv.utils import DDPM, set_seed, seed_worker\n",
    "from handsoncv.training import train_diffusion\n",
    "\n",
    "# Hardware & Paths\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\", \"..\"))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# Folders we frequently use across the experiments' notebooks\n",
    "ROOT_PATH = os.path.join(PROJECT_ROOT, \"Assignment-3\")\n",
    "ROOT_DATA = os.path.join(ROOT_PATH, \"data\")\n",
    "DATA_DIR = f\"{ROOT_DATA}/cropped_flowers\"\n",
    "SAMPLE_DIR = f\"{ROOT_DATA}/05_images\"\n",
    "CSV_PATH = f\"{ROOT_DATA}/clip_embeddings_metadata.csv\"\n",
    "\n",
    "CHECKPOINTS_DIR = os.path.join(ROOT_PATH, \"checkpoints\")\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "# Numpy and Torch Reproducibility\n",
    "SEED=42\n",
    "set_seed(42)\n",
    "\n",
    "# Base Configuration Parameters\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8c0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Metadata (Originate clip.csv)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(\"Generating CLIP metadata...\")\n",
    "    generate_clip_metadata(DATA_DIR, CSV_PATH, clip_model, clip_preprocess, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac786bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base transforms used by both training and validation data\n",
    "base_t = [\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "]\n",
    "\n",
    "# Training: Base + Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    *base_t \n",
    "])\n",
    "\n",
    "# Validation: Base only\n",
    "val_transform = transforms.Compose(base_t)\n",
    "\n",
    "# Cropped TF Flowers Data Loading\n",
    "# We use a dummy dataset just to get the total count\n",
    "temp_ds = TFflowersCLIPDataset(CSV_PATH)\n",
    "dataset_size = len(temp_ds)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(0.95 * dataset_size)\n",
    "\n",
    "# Shuffle indices once\n",
    "random.shuffle(indices)\n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "# Create a Generator object to pass to the dataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Create two separate Dataset Instances\n",
    "train_ds = Subset(TFflowersCLIPDataset(CSV_PATH, transform=train_transform), train_indices)\n",
    "val_ds = Subset(TFflowersCLIPDataset(CSV_PATH, transform=val_transform), val_indices)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, worker_init_fn=seed_worker, generator=g, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145dd37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility.\n",
      "Num params:  44900355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguarino-vanessa-emanuela\u001b[0m (\u001b[33mhandsoncv-research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/wandb/run-20260105_161828-0gvrrpza</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/0gvrrpza' target=\"_blank\">ddpm_unet_training</a></strong> to <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2' target=\"_blank\">https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/0gvrrpza' target=\"_blank\">https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/0gvrrpza</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.9444 | Val Loss: 0.4709\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep00.png\n",
      "Epoch 0: Val Loss: 0.4709 | CLIP Score: 0.1871\n",
      "Saved and logged samples for epoch 0\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 1: Train Loss: 0.3162 | Val Loss: 0.2457\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 2: Train Loss: 0.2413 | Val Loss: 0.2179\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 3: Train Loss: 0.1974 | Val Loss: 0.1980\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 4: Train Loss: 0.1823 | Val Loss: 0.1703\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 5: Train Loss: 0.1794 | Val Loss: 0.1739\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep05.png\n",
      "Epoch 5: Val Loss: 0.1739 | CLIP Score: 0.2059\n",
      "Saved and logged samples for epoch 5\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 6: Train Loss: 0.1645 | Val Loss: 0.1550\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 7: Train Loss: 0.1620 | Val Loss: 0.1336\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 8: Train Loss: 0.1486 | Val Loss: 0.1969\n",
      "Epoch 9: Train Loss: 0.1452 | Val Loss: 0.0914\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 10: Train Loss: 0.1441 | Val Loss: 0.1357\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep10.png\n",
      "Epoch 10: Val Loss: 0.1357 | CLIP Score: 0.2138\n",
      "Saved and logged samples for epoch 10\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 11: Train Loss: 0.1426 | Val Loss: 0.1207\n",
      "Epoch 12: Train Loss: 0.1279 | Val Loss: 0.1439\n",
      "Epoch 13: Train Loss: 0.1188 | Val Loss: 0.1117\n",
      "Epoch 14: Train Loss: 0.1327 | Val Loss: 0.1060\n",
      "Epoch 15: Train Loss: 0.1223 | Val Loss: 0.1122\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep15.png\n",
      "Epoch 15: Val Loss: 0.1122 | CLIP Score: 0.2062\n",
      "Saved and logged samples for epoch 15\n",
      "Epoch 16: Train Loss: 0.1128 | Val Loss: 0.1206\n",
      "Epoch 17: Train Loss: 0.1079 | Val Loss: 0.1112\n",
      "Epoch 18: Train Loss: 0.1118 | Val Loss: 0.1064\n",
      "Epoch 19: Train Loss: 0.1095 | Val Loss: 0.1301\n",
      "Epoch 20: Train Loss: 0.1059 | Val Loss: 0.1157\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep20.png\n",
      "Epoch 20: Val Loss: 0.1157 | CLIP Score: 0.2130\n",
      "Saved and logged samples for epoch 20\n",
      "Epoch 21: Train Loss: 0.1080 | Val Loss: 0.0961\n",
      "Epoch 22: Train Loss: 0.0997 | Val Loss: 0.1309\n",
      "Epoch 23: Train Loss: 0.0955 | Val Loss: 0.1025\n",
      "Epoch 24: Train Loss: 0.1091 | Val Loss: 0.0774\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 25: Train Loss: 0.0948 | Val Loss: 0.0814\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep25.png\n",
      "Epoch 25: Val Loss: 0.0814 | CLIP Score: 0.2178\n",
      "Saved and logged samples for epoch 25\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 26: Train Loss: 0.0969 | Val Loss: 0.1066\n",
      "Epoch 27: Train Loss: 0.0968 | Val Loss: 0.0940\n",
      "Epoch 28: Train Loss: 0.0961 | Val Loss: 0.0784\n",
      "Epoch 29: Train Loss: 0.0885 | Val Loss: 0.1016\n",
      "Epoch 30: Train Loss: 0.1087 | Val Loss: 0.1158\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep30.png\n",
      "Epoch 30: Val Loss: 0.1158 | CLIP Score: 0.2371\n",
      "Saved and logged samples for epoch 30\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 31: Train Loss: 0.1045 | Val Loss: 0.0673\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 32: Train Loss: 0.0954 | Val Loss: 0.1285\n",
      "Epoch 33: Train Loss: 0.0894 | Val Loss: 0.0816\n",
      "Epoch 34: Train Loss: 0.0953 | Val Loss: 0.1016\n",
      "Epoch 35: Train Loss: 0.0896 | Val Loss: 0.0983\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep35.png\n",
      "Epoch 35: Val Loss: 0.0983 | CLIP Score: 0.2322\n",
      "Saved and logged samples for epoch 35\n",
      "Epoch 36: Train Loss: 0.0900 | Val Loss: 0.0928\n",
      "Epoch 37: Train Loss: 0.0948 | Val Loss: 0.0872\n",
      "Epoch 38: Train Loss: 0.0931 | Val Loss: 0.0645\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 39: Train Loss: 0.0881 | Val Loss: 0.0939\n",
      "Epoch 40: Train Loss: 0.0985 | Val Loss: 0.0850\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep40.png\n",
      "Epoch 40: Val Loss: 0.0850 | CLIP Score: 0.2517\n",
      "Saved and logged samples for epoch 40\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 41: Train Loss: 0.0863 | Val Loss: 0.1049\n",
      "Epoch 42: Train Loss: 0.0886 | Val Loss: 0.0894\n",
      "Epoch 43: Train Loss: 0.0832 | Val Loss: 0.0740\n",
      "Epoch 44: Train Loss: 0.0823 | Val Loss: 0.0818\n",
      "Epoch 45: Train Loss: 0.0830 | Val Loss: 0.0656\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep45.png\n",
      "Epoch 45: Val Loss: 0.0656 | CLIP Score: 0.2621\n",
      "Saved and logged samples for epoch 45\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 46: Train Loss: 0.0916 | Val Loss: 0.0867\n",
      "Epoch 47: Train Loss: 0.0870 | Val Loss: 0.0822\n",
      "Epoch 48: Train Loss: 0.0787 | Val Loss: 0.0629\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 49: Train Loss: 0.0826 | Val Loss: 0.0690\n",
      "Epoch 50: Train Loss: 0.0874 | Val Loss: 0.1115\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep50.png\n",
      "Epoch 50: Val Loss: 0.1115 | CLIP Score: 0.2630\n",
      "Saved and logged samples for epoch 50\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 51: Train Loss: 0.0900 | Val Loss: 0.0820\n",
      "Epoch 52: Train Loss: 0.0814 | Val Loss: 0.0957\n",
      "Epoch 53: Train Loss: 0.0817 | Val Loss: 0.0961\n",
      "Epoch 54: Train Loss: 0.0775 | Val Loss: 0.0684\n",
      "Epoch 55: Train Loss: 0.0834 | Val Loss: 0.0792\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep55.png\n",
      "Epoch 55: Val Loss: 0.0792 | CLIP Score: 0.2613\n",
      "Saved and logged samples for epoch 55\n",
      "Epoch 56: Train Loss: 0.0804 | Val Loss: 0.0715\n",
      "Epoch 57: Train Loss: 0.0815 | Val Loss: 0.0956\n",
      "Epoch 58: Train Loss: 0.0928 | Val Loss: 0.0774\n",
      "Epoch 59: Train Loss: 0.0789 | Val Loss: 0.0713\n",
      "Epoch 60: Train Loss: 0.0868 | Val Loss: 0.0999\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep60.png\n",
      "Epoch 60: Val Loss: 0.0999 | CLIP Score: 0.2670\n",
      "Saved and logged samples for epoch 60\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 61: Train Loss: 0.0771 | Val Loss: 0.0962\n",
      "Epoch 62: Train Loss: 0.0798 | Val Loss: 0.0796\n",
      "Epoch 63: Train Loss: 0.0782 | Val Loss: 0.0714\n",
      "Epoch 64: Train Loss: 0.0823 | Val Loss: 0.0812\n",
      "Epoch 65: Train Loss: 0.0831 | Val Loss: 0.0878\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep65.png\n",
      "Epoch 65: Val Loss: 0.0878 | CLIP Score: 0.2891\n",
      "Saved and logged samples for epoch 65\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 66: Train Loss: 0.0771 | Val Loss: 0.0903\n",
      "Epoch 67: Train Loss: 0.0700 | Val Loss: 0.0681\n",
      "Epoch 68: Train Loss: 0.0813 | Val Loss: 0.0888\n",
      "Epoch 69: Train Loss: 0.0764 | Val Loss: 0.0651\n",
      "Epoch 70: Train Loss: 0.0718 | Val Loss: 0.0721\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep70.png\n",
      "Epoch 70: Val Loss: 0.0721 | CLIP Score: 0.2746\n",
      "Saved and logged samples for epoch 70\n",
      "Epoch 71: Train Loss: 0.0776 | Val Loss: 0.0503\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 72: Train Loss: 0.0787 | Val Loss: 0.0463\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 73: Train Loss: 0.0831 | Val Loss: 0.0937\n",
      "Epoch 74: Train Loss: 0.0726 | Val Loss: 0.0518\n",
      "Epoch 75: Train Loss: 0.0808 | Val Loss: 0.0631\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep75.png\n",
      "Epoch 75: Val Loss: 0.0631 | CLIP Score: 0.2775\n",
      "Saved and logged samples for epoch 75\n",
      "Epoch 76: Train Loss: 0.0788 | Val Loss: 0.0766\n",
      "Epoch 77: Train Loss: 0.0843 | Val Loss: 0.0683\n",
      "Epoch 78: Train Loss: 0.0820 | Val Loss: 0.0793\n",
      "Epoch 79: Train Loss: 0.0827 | Val Loss: 0.0639\n",
      "Epoch 80: Train Loss: 0.0766 | Val Loss: 0.0857\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep80.png\n",
      "Epoch 80: Val Loss: 0.0857 | CLIP Score: 0.2792\n",
      "Saved and logged samples for epoch 80\n",
      "Epoch 81: Train Loss: 0.0767 | Val Loss: 0.0687\n",
      "Epoch 82: Train Loss: 0.0741 | Val Loss: 0.0421\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 83: Train Loss: 0.0739 | Val Loss: 0.0569\n",
      "Epoch 84: Train Loss: 0.0752 | Val Loss: 0.0836\n",
      "Epoch 85: Train Loss: 0.0776 | Val Loss: 0.0624\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep85.png\n",
      "Epoch 85: Val Loss: 0.0624 | CLIP Score: 0.2770\n",
      "Saved and logged samples for epoch 85\n",
      "Epoch 86: Train Loss: 0.0777 | Val Loss: 0.0663\n",
      "Epoch 87: Train Loss: 0.0761 | Val Loss: 0.0810\n",
      "Epoch 88: Train Loss: 0.0714 | Val Loss: 0.0545\n",
      "Epoch 89: Train Loss: 0.0711 | Val Loss: 0.0624\n",
      "Epoch 90: Train Loss: 0.0810 | Val Loss: 0.0843\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep90.png\n",
      "Epoch 90: Val Loss: 0.0843 | CLIP Score: 0.2724\n",
      "Saved and logged samples for epoch 90\n",
      "Epoch 91: Train Loss: 0.0732 | Val Loss: 0.0763\n",
      "Epoch 92: Train Loss: 0.0778 | Val Loss: 0.0953\n",
      "Epoch 93: Train Loss: 0.0770 | Val Loss: 0.0772\n",
      "Epoch 94: Train Loss: 0.0761 | Val Loss: 0.0746\n",
      "Epoch 95: Train Loss: 0.0797 | Val Loss: 0.1219\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep95.png\n",
      "Epoch 95: Val Loss: 0.1219 | CLIP Score: 0.2828\n",
      "Saved and logged samples for epoch 95\n",
      "Epoch 96: Train Loss: 0.0739 | Val Loss: 0.0590\n",
      "Epoch 97: Train Loss: 0.0754 | Val Loss: 0.0524\n",
      "Epoch 98: Train Loss: 0.0781 | Val Loss: 0.0601\n",
      "Epoch 99: Train Loss: 0.0771 | Val Loss: 0.0653\n",
      "Epoch 100: Train Loss: 0.0773 | Val Loss: 0.0754\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep100.png\n",
      "Epoch 100: Val Loss: 0.0754 | CLIP Score: 0.2953\n",
      "Saved and logged samples for epoch 100\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 101: Train Loss: 0.0782 | Val Loss: 0.0777\n",
      "Epoch 102: Train Loss: 0.0780 | Val Loss: 0.0742\n",
      "Epoch 103: Train Loss: 0.0711 | Val Loss: 0.0873\n",
      "Epoch 104: Train Loss: 0.0706 | Val Loss: 0.0850\n",
      "Epoch 105: Train Loss: 0.0723 | Val Loss: 0.1015\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep105.png\n",
      "Epoch 105: Val Loss: 0.1015 | CLIP Score: 0.2835\n",
      "Saved and logged samples for epoch 105\n",
      "Epoch 106: Train Loss: 0.0806 | Val Loss: 0.0599\n",
      "Epoch 107: Train Loss: 0.0713 | Val Loss: 0.0720\n",
      "Epoch 108: Train Loss: 0.0732 | Val Loss: 0.0653\n",
      "Epoch 109: Train Loss: 0.0710 | Val Loss: 0.0535\n",
      "Epoch 110: Train Loss: 0.0808 | Val Loss: 0.0637\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep110.png\n",
      "Epoch 110: Val Loss: 0.0637 | CLIP Score: 0.2727\n",
      "Saved and logged samples for epoch 110\n",
      "Epoch 111: Train Loss: 0.0725 | Val Loss: 0.0747\n",
      "Epoch 112: Train Loss: 0.0699 | Val Loss: 0.0796\n",
      "Epoch 113: Train Loss: 0.0750 | Val Loss: 0.0747\n",
      "Epoch 114: Train Loss: 0.0696 | Val Loss: 0.0804\n",
      "Epoch 115: Train Loss: 0.0735 | Val Loss: 0.0820\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep115.png\n",
      "Epoch 115: Val Loss: 0.0820 | CLIP Score: 0.2748\n",
      "Saved and logged samples for epoch 115\n",
      "Epoch 116: Train Loss: 0.0687 | Val Loss: 0.1065\n",
      "Epoch 117: Train Loss: 0.0704 | Val Loss: 0.0612\n",
      "Epoch 118: Train Loss: 0.0747 | Val Loss: 0.0642\n",
      "Epoch 119: Train Loss: 0.0741 | Val Loss: 0.0542\n",
      "Epoch 120: Train Loss: 0.0656 | Val Loss: 0.0758\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep120.png\n",
      "Epoch 120: Val Loss: 0.0758 | CLIP Score: 0.2833\n",
      "Saved and logged samples for epoch 120\n",
      "Epoch 121: Train Loss: 0.0688 | Val Loss: 0.0570\n",
      "Epoch 122: Train Loss: 0.0775 | Val Loss: 0.0674\n",
      "Epoch 123: Train Loss: 0.0738 | Val Loss: 0.0658\n",
      "Epoch 124: Train Loss: 0.0710 | Val Loss: 0.0754\n",
      "Epoch 125: Train Loss: 0.0741 | Val Loss: 0.0528\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep125.png\n",
      "Epoch 125: Val Loss: 0.0528 | CLIP Score: 0.2813\n",
      "Saved and logged samples for epoch 125\n",
      "Epoch 126: Train Loss: 0.0761 | Val Loss: 0.0596\n",
      "Epoch 127: Train Loss: 0.0751 | Val Loss: 0.0508\n",
      "Epoch 128: Train Loss: 0.0668 | Val Loss: 0.0769\n",
      "Epoch 129: Train Loss: 0.0718 | Val Loss: 0.0645\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep129.png\n",
      "Epoch 129: Val Loss: 0.0645 | CLIP Score: 0.2846\n",
      "Saved and logged samples for epoch 129\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>clip_score</td><td>▁▂▃▂▃▃▄▄▅▆▆▆▆█▇▇▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>epoch_time_sec</td><td>█▁▁▇▁▁▁▁█▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▇▁▇▁▇▁▇▁▁▁▁▇▁▇</td></tr><tr><td>learning_rate</td><td>████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▃▂▂▂▂▁▂▂▂▁▁▁▁▂▁▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>clip_score</td><td>0.2846</td></tr><tr><td>epoch</td><td>129</td></tr><tr><td>epoch_time_sec</td><td>14.93703</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>peak_gpu_mem_mb</td><td>6112.83984</td></tr><tr><td>train_loss</td><td>0.07176</td></tr><tr><td>val_loss</td><td>0.06453</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ddpm_unet_training</strong> at: <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/0gvrrpza' target=\"_blank\">https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/0gvrrpza</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2' target=\"_blank\">https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2</a><br>Synced 4 W&B file(s), 157 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260105_161828-0gvrrpza/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training Configuration \n",
    "EPOCHS = 130\n",
    "LEARNING_RATE = 1e-4\n",
    "SUBSET_SIZE = len(train_ds) + len(val_ds) \n",
    "\n",
    "# Initialize Model & DDPM\n",
    "T = 400\n",
    "IMG_CH = 3\n",
    "IMG_SIZE = train_loader.dataset[0][0].shape[-1]\n",
    "BETAS = torch.linspace(0.0001, 0.02, T).to(DEVICE)\n",
    "# For OpenAI's CLIP, c_embed_dim is stored in model.visual.output_dim\n",
    "CLIP_EMBED_DIM = clip_model.visual.output_dim \n",
    "\n",
    "# Set Seed again for Ensuring Same Model Initialization at Every Run\n",
    "set_seed(SEED)\n",
    "\n",
    "ddpm = DDPM(BETAS, DEVICE)\n",
    "model = UNet(\n",
    "    T, \n",
    "    IMG_CH, \n",
    "    IMG_SIZE, \n",
    "    down_chs=(256, 256, 512), \n",
    "    t_embed_dim=8, \n",
    "    c_embed_dim=CLIP_EMBED_DIM\n",
    ").to(DEVICE)\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    'min', \n",
    "    patience=15, # We wait 15 epochs before cutting LR\n",
    "    factor=0.5,   # We don'tcut it so aggressively\n",
    "    min_lr=5e-5 # We stop the LR from dropping below 5e-5\n",
    ")\n",
    "BOTTLE_EMB_CHANNELS = model.down2.model[-2].model[0].out_channels\n",
    "\n",
    "# Define list of text prompts to generate images for \n",
    "text_list = [\n",
    "    \"A round white daisy with a yellow center\",\n",
    "    \"An orange sunflower with a big brown center\",\n",
    "    \"A deep red rose flower\"\n",
    "]\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"diffusion-model-assessment-v2\", \n",
    "    name=\"ddpm_unet_training\",\n",
    "    config={\n",
    "        \"architecture\": \"ddpm_unet\",\n",
    "        \"strategy\": \"generative_modeling\",\n",
    "        \"downsample_mode\": \"maxpool\",\n",
    "        \"embedding_size\": BOTTLE_EMB_CHANNELS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"optimizer_type\": \"Adam\",\n",
    "        \"subset_size\": SUBSET_SIZE,\n",
    "        \"seed\": SEED,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Execute Training\n",
    "train_diffusion(\n",
    "    model=model,\n",
    "    ddpm=ddpm,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    drop_prob=0.1,\n",
    "    save_dir=CHECKPOINTS_DIR,\n",
    "    sample_save_dir=SAMPLE_DIR,\n",
    "    clip_model=clip_model,   # Pass the clip model for evaluation\n",
    "    clip_preprocess=clip_preprocess,  # Pass the clip preprocess for evaluation\n",
    "    text_list=text_list,   # Pass the text prompts list for evaluation\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsoncv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

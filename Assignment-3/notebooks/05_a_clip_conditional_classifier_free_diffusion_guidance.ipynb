{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c20885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Seeds set to 42 for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "\n",
    "# On a multi-GPU system, this hides all GPUs except the first \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "# Custom modules\n",
    "from handsoncv.datasets import generate_clip_metadata, TFflowersCLIPDataset\n",
    "from handsoncv.models import UNet \n",
    "from handsoncv.utils import DDPM, set_seed, seed_worker\n",
    "from handsoncv.training import train_diffusion\n",
    "\n",
    "# Hardware & Paths\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\", \"..\"))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# Folders we frequently use across the experiments' notebooks\n",
    "ROOT_PATH = os.path.join(PROJECT_ROOT, \"Assignment-3\")\n",
    "ROOT_DATA = os.path.join(ROOT_PATH, \"data\")\n",
    "DATA_DIR = f\"{ROOT_DATA}/cropped_flowers\"\n",
    "SAMPLE_DIR = f\"{ROOT_DATA}/05_images\"\n",
    "CSV_PATH = f\"{ROOT_DATA}/clip_embeddings_metadata.csv\"\n",
    "\n",
    "CHECKPOINTS_DIR = os.path.join(ROOT_PATH, \"checkpoints\")\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "# Numpy and Torch Reproducibility\n",
    "SEED=42\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8c0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Metadata (Originate clip.csv)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(\"Generating CLIP metadata...\")\n",
    "    generate_clip_metadata(DATA_DIR, CSV_PATH, clip_model, clip_preprocess, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac786bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropped TF Flowers Data Loading\n",
    "full_dataset = TFflowersCLIPDataset(CSV_PATH, img_size=32)\n",
    "train_size = int(0.95 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create a Generator object to pass to the dataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=2, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145dd37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility.\n",
      "Num params:  44900355\n",
      "Epoch 0: Train Loss: 0.8968 | Val Loss: 0.5058\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep00.png\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 1: Train Loss: 0.3230 | Val Loss: 0.2326\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 2: Train Loss: 0.2180 | Val Loss: 0.1923\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 3: Train Loss: 0.1891 | Val Loss: 0.1900\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 4: Train Loss: 0.1787 | Val Loss: 0.1885\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 5: Train Loss: 0.1775 | Val Loss: 0.1586\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep05.png\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 6: Train Loss: 0.1716 | Val Loss: 0.1778\n",
      "Epoch 7: Train Loss: 0.1586 | Val Loss: 0.1757\n",
      "Epoch 8: Train Loss: 0.1632 | Val Loss: 0.1100\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 9: Train Loss: 0.1559 | Val Loss: 0.1211\n",
      "Epoch 10: Train Loss: 0.1452 | Val Loss: 0.1274\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep10.png\n",
      "Epoch 11: Train Loss: 0.1432 | Val Loss: 0.1413\n",
      "Epoch 12: Train Loss: 0.1392 | Val Loss: 0.1314\n",
      "Epoch 13: Train Loss: 0.1436 | Val Loss: 0.1464\n",
      "Epoch 14: Train Loss: 0.1455 | Val Loss: 0.1145\n",
      "Epoch 15: Train Loss: 0.1278 | Val Loss: 0.1257\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep15.png\n",
      "Epoch 16: Train Loss: 0.1275 | Val Loss: 0.0861\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 17: Train Loss: 0.1276 | Val Loss: 0.1371\n",
      "Epoch 18: Train Loss: 0.1206 | Val Loss: 0.1009\n",
      "Epoch 19: Train Loss: 0.1237 | Val Loss: 0.1742\n",
      "Epoch 20: Train Loss: 0.1144 | Val Loss: 0.1585\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep20.png\n",
      "Epoch 21: Train Loss: 0.1072 | Val Loss: 0.1551\n",
      "Epoch 22: Train Loss: 0.1132 | Val Loss: 0.1119\n",
      "Epoch 23: Train Loss: 0.1209 | Val Loss: 0.1088\n",
      "Epoch 24: Train Loss: 0.1035 | Val Loss: 0.1247\n",
      "Epoch 25: Train Loss: 0.1141 | Val Loss: 0.1135\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep25.png\n",
      "Epoch 26: Train Loss: 0.1004 | Val Loss: 0.1139\n",
      "Epoch 27: Train Loss: 0.1177 | Val Loss: 0.1232\n",
      "Epoch 28: Train Loss: 0.1084 | Val Loss: 0.0940\n",
      "Epoch 29: Train Loss: 0.1127 | Val Loss: 0.1358\n",
      "Epoch 30: Train Loss: 0.1031 | Val Loss: 0.0800\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep30.png\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 31: Train Loss: 0.1015 | Val Loss: 0.1135\n",
      "Epoch 32: Train Loss: 0.1033 | Val Loss: 0.0930\n",
      "Epoch 33: Train Loss: 0.1034 | Val Loss: 0.1258\n",
      "Epoch 34: Train Loss: 0.1012 | Val Loss: 0.0865\n",
      "Epoch 35: Train Loss: 0.0983 | Val Loss: 0.0754\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep35.png\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 36: Train Loss: 0.1063 | Val Loss: 0.1041\n",
      "Epoch 37: Train Loss: 0.0959 | Val Loss: 0.0745\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 38: Train Loss: 0.1059 | Val Loss: 0.1225\n",
      "Epoch 39: Train Loss: 0.0936 | Val Loss: 0.1312\n",
      "Epoch 40: Train Loss: 0.0964 | Val Loss: 0.1327\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep40.png\n",
      "Epoch 41: Train Loss: 0.0970 | Val Loss: 0.1238\n",
      "Epoch 42: Train Loss: 0.0988 | Val Loss: 0.0910\n",
      "Epoch 43: Train Loss: 0.0889 | Val Loss: 0.1027\n",
      "Epoch 44: Train Loss: 0.0982 | Val Loss: 0.0748\n",
      "Epoch 45: Train Loss: 0.0985 | Val Loss: 0.1231\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep45.png\n",
      "Epoch 46: Train Loss: 0.0902 | Val Loss: 0.0778\n",
      "Epoch 47: Train Loss: 0.0938 | Val Loss: 0.0663\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 48: Train Loss: 0.1019 | Val Loss: 0.1081\n",
      "Epoch 49: Train Loss: 0.0864 | Val Loss: 0.0799\n",
      "Epoch 50: Train Loss: 0.0858 | Val Loss: 0.0965\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep50.png\n",
      "Epoch 51: Train Loss: 0.0845 | Val Loss: 0.0931\n",
      "Epoch 52: Train Loss: 0.0934 | Val Loss: 0.0903\n",
      "Epoch 53: Train Loss: 0.0912 | Val Loss: 0.0856\n",
      "Epoch 54: Train Loss: 0.0894 | Val Loss: 0.1262\n",
      "Epoch 55: Train Loss: 0.0927 | Val Loss: 0.1329\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep55.png\n",
      "Epoch 56: Train Loss: 0.0902 | Val Loss: 0.1020\n",
      "Epoch 57: Train Loss: 0.0882 | Val Loss: 0.1093\n",
      "Epoch 58: Train Loss: 0.0873 | Val Loss: 0.1002\n",
      "Epoch 59: Train Loss: 0.0838 | Val Loss: 0.0613\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 60: Train Loss: 0.0963 | Val Loss: 0.0928\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep60.png\n",
      "Epoch 61: Train Loss: 0.0856 | Val Loss: 0.0802\n",
      "Epoch 62: Train Loss: 0.0891 | Val Loss: 0.1002\n",
      "Epoch 63: Train Loss: 0.0851 | Val Loss: 0.0862\n",
      "Epoch 64: Train Loss: 0.0844 | Val Loss: 0.0717\n",
      "Epoch 65: Train Loss: 0.0898 | Val Loss: 0.0556\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep65.png\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 66: Train Loss: 0.0904 | Val Loss: 0.0760\n",
      "Epoch 67: Train Loss: 0.0911 | Val Loss: 0.1035\n",
      "Epoch 68: Train Loss: 0.0810 | Val Loss: 0.0776\n",
      "Epoch 69: Train Loss: 0.0881 | Val Loss: 0.0811\n",
      "Epoch 70: Train Loss: 0.0848 | Val Loss: 0.0716\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep70.png\n",
      "Epoch 71: Train Loss: 0.0890 | Val Loss: 0.0995\n",
      "Epoch 72: Train Loss: 0.0851 | Val Loss: 0.0714\n",
      "Epoch 73: Train Loss: 0.0878 | Val Loss: 0.0861\n",
      "Epoch 74: Train Loss: 0.0854 | Val Loss: 0.0753\n",
      "Epoch 75: Train Loss: 0.0847 | Val Loss: 0.0714\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep75.png\n",
      "Epoch 76: Train Loss: 0.0840 | Val Loss: 0.0795\n",
      "Epoch 77: Train Loss: 0.0803 | Val Loss: 0.0689\n",
      "Epoch 78: Train Loss: 0.0796 | Val Loss: 0.0708\n",
      "Epoch 79: Train Loss: 0.0815 | Val Loss: 0.0947\n",
      "Epoch 80: Train Loss: 0.0801 | Val Loss: 0.0815\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep80.png\n",
      "Epoch 81: Train Loss: 0.0818 | Val Loss: 0.0586\n",
      "Epoch 82: Train Loss: 0.0809 | Val Loss: 0.1132\n",
      "Epoch 83: Train Loss: 0.0842 | Val Loss: 0.0699\n",
      "Epoch 84: Train Loss: 0.0779 | Val Loss: 0.0826\n",
      "Epoch 85: Train Loss: 0.0803 | Val Loss: 0.0916\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep85.png\n",
      "Epoch 86: Train Loss: 0.0827 | Val Loss: 0.0740\n",
      "Epoch 87: Train Loss: 0.0787 | Val Loss: 0.0974\n",
      "Epoch 88: Train Loss: 0.0824 | Val Loss: 0.0576\n",
      "Epoch 89: Train Loss: 0.0853 | Val Loss: 0.0468\n",
      "--- Saved new best model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 90: Train Loss: 0.0801 | Val Loss: 0.0794\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep90.png\n",
      "Epoch 91: Train Loss: 0.0805 | Val Loss: 0.0651\n",
      "Epoch 92: Train Loss: 0.0787 | Val Loss: 0.0861\n",
      "Epoch 93: Train Loss: 0.0785 | Val Loss: 0.0497\n",
      "Epoch 94: Train Loss: 0.0776 | Val Loss: 0.0678\n",
      "Epoch 95: Train Loss: 0.0801 | Val Loss: 0.0852\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep95.png\n",
      "Epoch 96: Train Loss: 0.0821 | Val Loss: 0.0561\n",
      "Epoch 97: Train Loss: 0.0753 | Val Loss: 0.0616\n",
      "Epoch 98: Train Loss: 0.0780 | Val Loss: 0.0553\n",
      "Epoch 99: Train Loss: 0.0796 | Val Loss: 0.0872\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep99.png\n"
     ]
    }
   ],
   "source": [
    "# Configuration \n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "SUBSET_SIZE = len(train_ds) + len(val_ds) \n",
    "\n",
    "# Initialize Model & DDPM\n",
    "T = 400\n",
    "IMG_CH = 3\n",
    "IMG_SIZE = train_loader.dataset[0][0].shape[-1]\n",
    "BETAS = torch.linspace(0.0001, 0.02, T).to(DEVICE)\n",
    "# For OpenAI's CLIP, c_embed_dim is stored in model.visual.output_dim\n",
    "CLIP_EMBED_DIM = clip_model.visual.output_dim \n",
    "\n",
    "# Set Seed again for Ensuring Same Model Initialization at Every Run\n",
    "set_seed(SEED)\n",
    "\n",
    "ddpm = DDPM(BETAS, DEVICE)\n",
    "model = UNet(\n",
    "    T, \n",
    "    IMG_CH, \n",
    "    IMG_SIZE, \n",
    "    down_chs=(256, 256, 512), \n",
    "    t_embed_dim=8, \n",
    "    c_embed_dim=CLIP_EMBED_DIM\n",
    ").to(DEVICE)\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Define list of text prompts to geenrate images for \n",
    "text_list = [\n",
    "    \"A round white daisy with a yellow center\",\n",
    "    \"An orange sunflower with a big brown center\",\n",
    "    \"A deep red rose flower\"\n",
    "]\n",
    "\n",
    "# Execute Training\n",
    "train_diffusion(\n",
    "    model=model,\n",
    "    ddpm=ddpm,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    drop_prob=0.1,\n",
    "    save_dir=CHECKPOINTS_DIR,\n",
    "    sample_save_dir=SAMPLE_DIR,\n",
    "    clip_model=clip_model,   # Pass the clip model for evaluation\n",
    "    text_list=text_list   # Pass the text prompts list for evaluation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsoncv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

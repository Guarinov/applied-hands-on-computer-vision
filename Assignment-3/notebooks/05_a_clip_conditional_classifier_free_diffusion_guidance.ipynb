{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2dbd01",
   "metadata": {},
   "source": [
    "## Text-to-Image Conditional Diffusion with Classifier-Free Guidance\n",
    "\n",
    "In this notebook, we will train a **text-to-image conditional diffusion model** on a cropped version of the **TF Flowers** dataset. The goal is to generate flower images conditioned on semantic information while retaining the ability to generate unconditioned samples.\n",
    "\n",
    "Our model combines:\n",
    "- A **CLIP text encoder** for textual conditioning;\n",
    "- A **U-Net–based DDPM** for image generation;\n",
    "- **Classifier-Free Diffusion Guidance (CFG)** to strengthen conditioning at sampling time without requiring a separate classifier.\n",
    "\n",
    "To this end, we train a Conditional Diffusion Model, where the image denoising network is conditioned on both the diffusion timestep $t$, embedded using a sinusoidal embedding, and a context embedding $c$ derived from text. The overall architecture follows the **Denoising Diffusion Probabilistic Models (DDPM)** framework ([Ho et al., 2020](https://arxiv.org/pdf/2006.11239)), with a modified U-Net that supports conditional inputs. As in standard diffusion models, timestep embeddings provide the model with information about the current noise level during denoising.\n",
    "\n",
    "To condition the model, we inject text-derived embeddings, represented by the context vector $c$, into the U-Net. Sample quality is further improved via CFG, which allows us to control how strongly the conditioning influences generation at inference time. To enable this mechanism, the model is trained to operate both _with conditioning_ and _without conditioning_. This is achieved by randomly masking the context input during training using a Bernoulli distribution. During sampling, noise predictions with and without conditioning are linearly combined using a _guidance weight_, and this amplified noise estimate is then used in the reverse diffusion process.\n",
    "\n",
    "In the following cell, we import the modules and functions defined in our `src` package, set random seeds for reproducibility, and define the parameters and file paths used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c20885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Seeds set to 42 for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import wandb\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# On a multi-GPU system, this hides all GPUs except the first \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "# Custom modules\n",
    "from handsoncv.datasets import generate_clip_metadata, TFflowersCLIPDataset\n",
    "from handsoncv.models import UNet \n",
    "from handsoncv.utils import DDPM, set_seed, seed_worker\n",
    "from handsoncv.training import train_diffusion\n",
    "\n",
    "# Hardware & Paths\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\", \"..\"))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# Folders we frequently use across the experiments' notebooks\n",
    "ROOT_PATH = os.path.join(PROJECT_ROOT, \"Assignment-3\")\n",
    "ROOT_DATA = os.path.join(ROOT_PATH, \"data\")\n",
    "DATA_DIR = f\"{ROOT_DATA}/cropped_flowers\"\n",
    "SAMPLE_DIR = f\"{ROOT_DATA}/05_flowers_images\"\n",
    "CSV_PATH = f\"{ROOT_DATA}/clip_embeddings_metadata.csv\"\n",
    "\n",
    "CHECKPOINTS_DIR = os.path.join(ROOT_PATH, \"checkpoints\")\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "# Numpy and Torch Reproducibility\n",
    "SEED=42\n",
    "set_seed(42)\n",
    "\n",
    "# Base Configuration Parameters\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5025e",
   "metadata": {},
   "source": [
    "### Data Preparation \n",
    "\n",
    "In this cell, we load a pretrained `CLIP ViT-B/32` model and use it to precompute image embeddings for the entire training dataset. For each image, we extract its CLIP image embedding and store it, together with the image path, in a `.csv` file located in the data directory.\n",
    "\n",
    "This preprocessing step is performed once and avoids repeatedly encoding images with CLIP during training, which would be computationally expensive. If the metadata file already exists, this step is skipped to ensure reproducibility and reduce unnecessary computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8c0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Metadata (Originate clip.csv)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(\"Generating CLIP metadata...\")\n",
    "    generate_clip_metadata(DATA_DIR, CSV_PATH, clip_model, clip_preprocess, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f82652",
   "metadata": {},
   "source": [
    "We train on a modified version of the **TF Flowers datase**t consisting of cropped color images focused on the flower itself. These crops reduce background noise and encourage the model to focus on shape, color and texture of flowers. The dataset was provided as part of NVIDIA’s course [Generative AI with Diffusion Models](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+C-FX-08+V1) and was downloaded as a zipped archive via a shared Google Drive link.\n",
    "\n",
    "The reduced version of the dataset contains only three classes (daisies, roses, and sunflowers) and includes roughly 1100 images in total. Given this limited dataset size, the model cannot fully capture the true generative distribution of natural flower images. To mitigate overfitting and improve generalization, we apply horizontal flipping as data augmentation technique during training.\n",
    "\n",
    "---\n",
    "\n",
    "All images are resized to 32×32 pixels, following NVIDIA’s reference implementation and model constraints. While this enables efficient training, it also leads to a significant loss of fine-grained spatial detail, which further limits the model’s ability to learn high-frequency structures and detailed textures.\n",
    "\n",
    "The dataset is split into 95% training and 5% validation subsets using a fixed random seed to ensure reproducibility. The split is performed once by shuffling dataset indices and creating separate Subset instances for training and validation. \n",
    "\n",
    "Images are normalized to the range [-1, 1], which matches the input assumptions of the diffusion model. Training data uses both base preprocessing and augmentation, while validation data uses only deterministic base transforms. The custom `TFflowersCLIPDataset` returns a transformed RGB image tensor of shape $(3, 32, 32)$ and the corresponding CLIP embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac786bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base transforms used by both training and validation data\n",
    "base_t = [\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "]\n",
    "\n",
    "# Training: Base + Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # Augmentation present in Nvidia's notebook\n",
    "    *base_t \n",
    "])\n",
    "\n",
    "# Validation: Base only\n",
    "val_transform = transforms.Compose(base_t)\n",
    "\n",
    "# Cropped TF Flowers Data Loading\n",
    "# We use a dummy dataset just to get the total count\n",
    "temp_ds = TFflowersCLIPDataset(CSV_PATH)\n",
    "dataset_size = len(temp_ds)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(0.95 * dataset_size)\n",
    "\n",
    "# Shuffle indices once\n",
    "random.shuffle(indices)\n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "# Create a Generator object to pass to the dataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Create two separate Dataset Instances\n",
    "train_ds = Subset(TFflowersCLIPDataset(CSV_PATH, transform=train_transform), train_indices)\n",
    "val_ds = Subset(TFflowersCLIPDataset(CSV_PATH, transform=val_transform), val_indices)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, worker_init_fn=seed_worker, generator=g, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0149d16",
   "metadata": {},
   "source": [
    "### Training the UNet–DDPM Model\n",
    "\n",
    "In this section, we configure and train a **conditional UNet-based DDPM** for image generation. We define the training hyperparameters, initialize the diffusion process, construct the UNet architecture, and launch the training loop. The diffusion process uses 400 timesteps with a linear noise schedule, and conditioning is provided via CLIP image embeddings.\n",
    "\n",
    "To ensure reproducibility, random seeds are reset prior to model initialization. Training is performed using the Adam optimizer with a learning rate scheduler (`ReduceLROnPlateau`) that adaptively reduces the learning rate when validation loss stagnates. During training, CFR is enabled by randomly dropping conditioning information with a fixed probability. . Periodically, the model generates samples from a small set of text prompts to qualitatively monitor progress. All training statistics and metrics are logged to [diffusion-model-assessment-v2](https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/workspace); for reference, see the runs labeled `*-training`, especially the pinned ones.\n",
    "\n",
    "---\n",
    "\n",
    "#### UNet-DDPM Architecture\n",
    "\n",
    "The model follows a UNet architecture ([Ronneberger et al., 2015](https://arxiv.org/pdf/1505.04597)) tailored for diffusion-based generative modeling and conditional image synthesis.\n",
    "\n",
    "1. Encoder (Downsampling Path):\n",
    "- An initial residual convolution block projects the input image into feature space.\n",
    "- Two downsampling stages composed of:\n",
    "  - Convolutional blocks with **Group Normalization** and **GELU** activations.\n",
    "  - Spatial downsampling via **rearrangement-based pooling**.\n",
    "\n",
    "2. Bottleneck:\n",
    "- Operates directly on a **low-resolution spatial feature map** (e.g., 8×8) rather than flattening features into a vector.\n",
    "- Optionally includes a **self-attention block** to capture long-range spatial dependencies.\n",
    "\n",
    "3. Conditioning Mechanism:\n",
    "- Diffusion timesteps are embedded using **sinusoidal positional encodings**.\n",
    "- CLIP conditioning vectors are projected into spatial embeddings using learned embedding blocks.\n",
    "- Conditioning is injected via **scale-and-shift modulation** during the upsampling stages.\n",
    "- A Bernoulli mask enables **classifier-free training** by randomly removing conditioning information.\n",
    "\n",
    "4. Decoder (Upsampling Path):\n",
    "- Upsampling is performed using **nearest-neighbor interpolation followed by convolution**, avoiding transposed convolutions.\n",
    "- Skip connections from the encoder preserve fine-grained spatial details.\n",
    "- A final convolution maps features back to RGB space, producing a **noise prediction** at each diffusion timestep.\n",
    "\n",
    "> **Note.**\n",
    "> Compared to the NVIDIA reference model, this implementation preserves the bottleneck as a 2D spatial feature map instead of flattening it and reprojecting with an MLP, which helps maintain spatial coherence. An optional multi-head self-attention block is added at the bottleneck to capture global spatial dependencies. Additionally, transposed convolutions are replaced with nearest-neighbor upsampling followed by convolution to improve training stability and reduce checkerboard artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145dd37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility.\n",
      "Num params:  34122243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguarino-vanessa-emanuela\u001b[0m (\u001b[33mhandsoncv-research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/wandb/run-20260118_001259-dds30dl5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/dds30dl5' target=\"_blank\">ddpm_unet_training</a></strong> to <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2' target=\"_blank\">https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/dds30dl5' target=\"_blank\">https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/dds30dl5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.9390 | Val Loss: 0.4279\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep00.png\n",
      "Epoch 0: Val Loss: 0.4279 | CLIP Score: 0.1886\n",
      "Saved and logged samples for epoch 0\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 1: Train Loss: 0.2770 | Val Loss: 0.1984\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 2: Train Loss: 0.2042 | Val Loss: 0.1700\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 3: Train Loss: 0.1730 | Val Loss: 0.1813\n",
      "Epoch 4: Train Loss: 0.1629 | Val Loss: 0.1491\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 5: Train Loss: 0.1578 | Val Loss: 0.1546\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep05.png\n",
      "Epoch 5: Val Loss: 0.1546 | CLIP Score: 0.1953\n",
      "Saved and logged samples for epoch 5\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 6: Train Loss: 0.1433 | Val Loss: 0.1306\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 7: Train Loss: 0.1371 | Val Loss: 0.1129\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 8: Train Loss: 0.1258 | Val Loss: 0.1668\n",
      "Epoch 9: Train Loss: 0.1255 | Val Loss: 0.0775\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 10: Train Loss: 0.1243 | Val Loss: 0.1138\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep10.png\n",
      "Epoch 10: Val Loss: 0.1138 | CLIP Score: 0.2071\n",
      "Saved and logged samples for epoch 10\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 11: Train Loss: 0.1266 | Val Loss: 0.1063\n",
      "Epoch 12: Train Loss: 0.1162 | Val Loss: 0.1385\n",
      "Epoch 13: Train Loss: 0.1093 | Val Loss: 0.0942\n",
      "Epoch 14: Train Loss: 0.1176 | Val Loss: 0.0890\n",
      "Epoch 15: Train Loss: 0.1053 | Val Loss: 0.0992\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep15.png\n",
      "Epoch 15: Val Loss: 0.0992 | CLIP Score: 0.1846\n",
      "Saved and logged samples for epoch 15\n",
      "Epoch 16: Train Loss: 0.1000 | Val Loss: 0.1113\n",
      "Epoch 17: Train Loss: 0.0999 | Val Loss: 0.1030\n",
      "Epoch 18: Train Loss: 0.1035 | Val Loss: 0.0980\n",
      "Epoch 19: Train Loss: 0.0998 | Val Loss: 0.1138\n",
      "Epoch 20: Train Loss: 0.0926 | Val Loss: 0.1020\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep20.png\n",
      "Epoch 20: Val Loss: 0.1020 | CLIP Score: 0.2185\n",
      "Saved and logged samples for epoch 20\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 21: Train Loss: 0.0953 | Val Loss: 0.0855\n",
      "Epoch 22: Train Loss: 0.0892 | Val Loss: 0.1179\n",
      "Epoch 23: Train Loss: 0.0872 | Val Loss: 0.0941\n",
      "Epoch 24: Train Loss: 0.1002 | Val Loss: 0.0701\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 25: Train Loss: 0.0871 | Val Loss: 0.0741\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep25.png\n",
      "Epoch 25: Val Loss: 0.0741 | CLIP Score: 0.2190\n",
      "Saved and logged samples for epoch 25\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 26: Train Loss: 0.0895 | Val Loss: 0.0979\n",
      "Epoch 27: Train Loss: 0.0889 | Val Loss: 0.0850\n",
      "Epoch 28: Train Loss: 0.0903 | Val Loss: 0.0737\n",
      "Epoch 29: Train Loss: 0.0828 | Val Loss: 0.0922\n",
      "Epoch 30: Train Loss: 0.0959 | Val Loss: 0.1028\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep30.png\n",
      "Epoch 30: Val Loss: 0.1028 | CLIP Score: 0.2211\n",
      "Saved and logged samples for epoch 30\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 31: Train Loss: 0.0948 | Val Loss: 0.0635\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 32: Train Loss: 0.0895 | Val Loss: 0.1208\n",
      "Epoch 33: Train Loss: 0.0845 | Val Loss: 0.0788\n",
      "Epoch 34: Train Loss: 0.0911 | Val Loss: 0.0992\n",
      "Epoch 35: Train Loss: 0.0873 | Val Loss: 0.0965\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep35.png\n",
      "Epoch 35: Val Loss: 0.0965 | CLIP Score: 0.2418\n",
      "Saved and logged samples for epoch 35\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 36: Train Loss: 0.0879 | Val Loss: 0.0904\n",
      "Epoch 37: Train Loss: 0.0916 | Val Loss: 0.0823\n",
      "Epoch 38: Train Loss: 0.0865 | Val Loss: 0.0614\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 39: Train Loss: 0.0836 | Val Loss: 0.0894\n",
      "Epoch 40: Train Loss: 0.0951 | Val Loss: 0.0824\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep40.png\n",
      "Epoch 40: Val Loss: 0.0824 | CLIP Score: 0.2363\n",
      "Saved and logged samples for epoch 40\n",
      "Epoch 41: Train Loss: 0.0826 | Val Loss: 0.0995\n",
      "Epoch 42: Train Loss: 0.0852 | Val Loss: 0.0885\n",
      "Epoch 43: Train Loss: 0.0815 | Val Loss: 0.0738\n",
      "Epoch 44: Train Loss: 0.0805 | Val Loss: 0.0806\n",
      "Epoch 45: Train Loss: 0.0801 | Val Loss: 0.0643\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep45.png\n",
      "Epoch 45: Val Loss: 0.0643 | CLIP Score: 0.2501\n",
      "Saved and logged samples for epoch 45\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 46: Train Loss: 0.0881 | Val Loss: 0.0854\n",
      "Epoch 47: Train Loss: 0.0849 | Val Loss: 0.0813\n",
      "Epoch 48: Train Loss: 0.0782 | Val Loss: 0.0628\n",
      "Epoch 49: Train Loss: 0.0812 | Val Loss: 0.0687\n",
      "Epoch 50: Train Loss: 0.0853 | Val Loss: 0.1063\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep50.png\n",
      "Epoch 50: Val Loss: 0.1063 | CLIP Score: 0.2560\n",
      "Saved and logged samples for epoch 50\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 51: Train Loss: 0.0873 | Val Loss: 0.0796\n",
      "Epoch 52: Train Loss: 0.0801 | Val Loss: 0.0917\n",
      "Epoch 53: Train Loss: 0.0799 | Val Loss: 0.0916\n",
      "Epoch 54: Train Loss: 0.0753 | Val Loss: 0.0685\n",
      "Epoch 55: Train Loss: 0.0813 | Val Loss: 0.0762\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep55.png\n",
      "Epoch 55: Val Loss: 0.0762 | CLIP Score: 0.2633\n",
      "Saved and logged samples for epoch 55\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 56: Train Loss: 0.0782 | Val Loss: 0.0685\n",
      "Epoch 57: Train Loss: 0.0797 | Val Loss: 0.0927\n",
      "Epoch 58: Train Loss: 0.0888 | Val Loss: 0.0746\n",
      "Epoch 59: Train Loss: 0.0753 | Val Loss: 0.0689\n",
      "Epoch 60: Train Loss: 0.0842 | Val Loss: 0.0982\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep60.png\n",
      "Epoch 60: Val Loss: 0.0982 | CLIP Score: 0.2658\n",
      "Saved and logged samples for epoch 60\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 61: Train Loss: 0.0750 | Val Loss: 0.0948\n",
      "Epoch 62: Train Loss: 0.0778 | Val Loss: 0.0775\n",
      "Epoch 63: Train Loss: 0.0765 | Val Loss: 0.0694\n",
      "Epoch 64: Train Loss: 0.0807 | Val Loss: 0.0791\n",
      "Epoch 65: Train Loss: 0.0818 | Val Loss: 0.0881\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep65.png\n",
      "Epoch 65: Val Loss: 0.0881 | CLIP Score: 0.2715\n",
      "Saved and logged samples for epoch 65\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 66: Train Loss: 0.0763 | Val Loss: 0.0888\n",
      "Epoch 67: Train Loss: 0.0697 | Val Loss: 0.0684\n",
      "Epoch 68: Train Loss: 0.0810 | Val Loss: 0.0896\n",
      "Epoch 69: Train Loss: 0.0762 | Val Loss: 0.0644\n",
      "Epoch 70: Train Loss: 0.0718 | Val Loss: 0.0729\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep70.png\n",
      "Epoch 70: Val Loss: 0.0729 | CLIP Score: 0.2630\n",
      "Saved and logged samples for epoch 70\n",
      "Epoch 71: Train Loss: 0.0775 | Val Loss: 0.0513\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 72: Train Loss: 0.0782 | Val Loss: 0.0465\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 73: Train Loss: 0.0827 | Val Loss: 0.0925\n",
      "Epoch 74: Train Loss: 0.0727 | Val Loss: 0.0526\n",
      "Epoch 75: Train Loss: 0.0808 | Val Loss: 0.0640\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep75.png\n",
      "Epoch 75: Val Loss: 0.0640 | CLIP Score: 0.2667\n",
      "Saved and logged samples for epoch 75\n",
      "Epoch 76: Train Loss: 0.0786 | Val Loss: 0.0778\n",
      "Epoch 77: Train Loss: 0.0843 | Val Loss: 0.0679\n",
      "Epoch 78: Train Loss: 0.0811 | Val Loss: 0.0788\n",
      "Epoch 79: Train Loss: 0.0819 | Val Loss: 0.0638\n",
      "Epoch 80: Train Loss: 0.0763 | Val Loss: 0.0858\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep80.png\n",
      "Epoch 80: Val Loss: 0.0858 | CLIP Score: 0.2595\n",
      "Saved and logged samples for epoch 80\n",
      "Epoch 81: Train Loss: 0.0769 | Val Loss: 0.0688\n",
      "Epoch 82: Train Loss: 0.0738 | Val Loss: 0.0427\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 83: Train Loss: 0.0739 | Val Loss: 0.0577\n",
      "Epoch 84: Train Loss: 0.0754 | Val Loss: 0.0834\n",
      "Epoch 85: Train Loss: 0.0775 | Val Loss: 0.0634\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep85.png\n",
      "Epoch 85: Val Loss: 0.0634 | CLIP Score: 0.2707\n",
      "Saved and logged samples for epoch 85\n",
      "Epoch 86: Train Loss: 0.0778 | Val Loss: 0.0682\n",
      "Epoch 87: Train Loss: 0.0758 | Val Loss: 0.0806\n",
      "Epoch 88: Train Loss: 0.0712 | Val Loss: 0.0546\n",
      "Epoch 89: Train Loss: 0.0710 | Val Loss: 0.0630\n",
      "Epoch 90: Train Loss: 0.0807 | Val Loss: 0.0849\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep90.png\n",
      "Epoch 90: Val Loss: 0.0849 | CLIP Score: 0.2650\n",
      "Saved and logged samples for epoch 90\n",
      "Epoch 91: Train Loss: 0.0734 | Val Loss: 0.0760\n",
      "Epoch 92: Train Loss: 0.0777 | Val Loss: 0.0966\n",
      "Epoch 93: Train Loss: 0.0773 | Val Loss: 0.0778\n",
      "Epoch 94: Train Loss: 0.0766 | Val Loss: 0.0757\n",
      "Epoch 95: Train Loss: 0.0803 | Val Loss: 0.1204\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep95.png\n",
      "Epoch 95: Val Loss: 0.1204 | CLIP Score: 0.2698\n",
      "Saved and logged samples for epoch 95\n",
      "Epoch 96: Train Loss: 0.0746 | Val Loss: 0.0599\n",
      "Epoch 97: Train Loss: 0.0757 | Val Loss: 0.0531\n",
      "Epoch 98: Train Loss: 0.0780 | Val Loss: 0.0603\n",
      "Epoch 99: Train Loss: 0.0768 | Val Loss: 0.0648\n",
      "Epoch 100: Train Loss: 0.0774 | Val Loss: 0.0762\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep100.png\n",
      "Epoch 100: Val Loss: 0.0762 | CLIP Score: 0.2759\n",
      "Saved and logged samples for epoch 100\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 101: Train Loss: 0.0782 | Val Loss: 0.0765\n",
      "Epoch 102: Train Loss: 0.0782 | Val Loss: 0.0748\n",
      "Epoch 103: Train Loss: 0.0716 | Val Loss: 0.0872\n",
      "Epoch 104: Train Loss: 0.0705 | Val Loss: 0.0847\n",
      "Epoch 105: Train Loss: 0.0724 | Val Loss: 0.1015\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep105.png\n",
      "Epoch 105: Val Loss: 0.1015 | CLIP Score: 0.2698\n",
      "Saved and logged samples for epoch 105\n",
      "Epoch 106: Train Loss: 0.0803 | Val Loss: 0.0599\n",
      "Epoch 107: Train Loss: 0.0717 | Val Loss: 0.0729\n",
      "Epoch 108: Train Loss: 0.0735 | Val Loss: 0.0660\n",
      "Epoch 109: Train Loss: 0.0714 | Val Loss: 0.0546\n",
      "Epoch 110: Train Loss: 0.0804 | Val Loss: 0.0642\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep110.png\n",
      "Epoch 110: Val Loss: 0.0642 | CLIP Score: 0.2698\n",
      "Saved and logged samples for epoch 110\n",
      "Epoch 111: Train Loss: 0.0723 | Val Loss: 0.0736\n",
      "Epoch 112: Train Loss: 0.0698 | Val Loss: 0.0784\n",
      "Epoch 113: Train Loss: 0.0752 | Val Loss: 0.0750\n",
      "Epoch 114: Train Loss: 0.0698 | Val Loss: 0.0818\n",
      "Epoch 115: Train Loss: 0.0734 | Val Loss: 0.0824\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep115.png\n",
      "Epoch 115: Val Loss: 0.0824 | CLIP Score: 0.2780\n",
      "Saved and logged samples for epoch 115\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 116: Train Loss: 0.0691 | Val Loss: 0.1067\n",
      "Epoch 117: Train Loss: 0.0712 | Val Loss: 0.0621\n",
      "Epoch 118: Train Loss: 0.0753 | Val Loss: 0.0651\n",
      "Epoch 119: Train Loss: 0.0745 | Val Loss: 0.0542\n",
      "Epoch 120: Train Loss: 0.0659 | Val Loss: 0.0757\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep120.png\n",
      "Epoch 120: Val Loss: 0.0757 | CLIP Score: 0.2797\n",
      "Saved and logged samples for epoch 120\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 121: Train Loss: 0.0691 | Val Loss: 0.0573\n",
      "Epoch 122: Train Loss: 0.0776 | Val Loss: 0.0684\n",
      "Epoch 123: Train Loss: 0.0744 | Val Loss: 0.0665\n",
      "Epoch 124: Train Loss: 0.0715 | Val Loss: 0.0749\n",
      "Epoch 125: Train Loss: 0.0748 | Val Loss: 0.0532\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep125.png\n",
      "Epoch 125: Val Loss: 0.0532 | CLIP Score: 0.2703\n",
      "Saved and logged samples for epoch 125\n",
      "Epoch 126: Train Loss: 0.0763 | Val Loss: 0.0597\n",
      "Epoch 127: Train Loss: 0.0749 | Val Loss: 0.0507\n",
      "Epoch 128: Train Loss: 0.0669 | Val Loss: 0.0751\n",
      "Epoch 129: Train Loss: 0.0716 | Val Loss: 0.0646\n",
      "Epoch 130: Train Loss: 0.0733 | Val Loss: 0.0407\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep130.png\n",
      "Epoch 130: Val Loss: 0.0407 | CLIP Score: 0.2642\n",
      "Saved and logged samples for epoch 130\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 131: Train Loss: 0.0728 | Val Loss: 0.0835\n",
      "Epoch 132: Train Loss: 0.0712 | Val Loss: 0.0777\n",
      "Epoch 133: Train Loss: 0.0741 | Val Loss: 0.0668\n",
      "Epoch 134: Train Loss: 0.0718 | Val Loss: 0.0728\n",
      "Epoch 135: Train Loss: 0.0685 | Val Loss: 0.0339\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep135.png\n",
      "Epoch 135: Val Loss: 0.0339 | CLIP Score: 0.2694\n",
      "Saved and logged samples for epoch 135\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 136: Train Loss: 0.0775 | Val Loss: 0.0738\n",
      "Epoch 137: Train Loss: 0.0705 | Val Loss: 0.0815\n",
      "Epoch 138: Train Loss: 0.0726 | Val Loss: 0.0606\n",
      "Epoch 139: Train Loss: 0.0714 | Val Loss: 0.0888\n",
      "Epoch 140: Train Loss: 0.0737 | Val Loss: 0.0589\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep140.png\n",
      "Epoch 140: Val Loss: 0.0589 | CLIP Score: 0.2812\n",
      "Saved and logged samples for epoch 140\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 141: Train Loss: 0.0766 | Val Loss: 0.0691\n",
      "Epoch 142: Train Loss: 0.0702 | Val Loss: 0.0704\n",
      "Epoch 143: Train Loss: 0.0659 | Val Loss: 0.0570\n",
      "Epoch 144: Train Loss: 0.0719 | Val Loss: 0.0716\n",
      "Epoch 145: Train Loss: 0.0741 | Val Loss: 0.0629\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep145.png\n",
      "Epoch 145: Val Loss: 0.0629 | CLIP Score: 0.2771\n",
      "Saved and logged samples for epoch 145\n",
      "Epoch 146: Train Loss: 0.0721 | Val Loss: 0.0954\n",
      "Epoch 147: Train Loss: 0.0722 | Val Loss: 0.0662\n",
      "Epoch 148: Train Loss: 0.0730 | Val Loss: 0.0594\n",
      "Epoch 149: Train Loss: 0.0665 | Val Loss: 0.0550\n",
      "Epoch 150: Train Loss: 0.0745 | Val Loss: 0.0540\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep150.png\n",
      "Epoch 150: Val Loss: 0.0540 | CLIP Score: 0.2773\n",
      "Saved and logged samples for epoch 150\n",
      "Epoch 151: Train Loss: 0.0753 | Val Loss: 0.0700\n",
      "Epoch 152: Train Loss: 0.0676 | Val Loss: 0.0611\n",
      "Epoch 153: Train Loss: 0.0699 | Val Loss: 0.0617\n",
      "Epoch 154: Train Loss: 0.0709 | Val Loss: 0.0651\n",
      "Epoch 155: Train Loss: 0.0706 | Val Loss: 0.0489\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep155.png\n",
      "Epoch 155: Val Loss: 0.0489 | CLIP Score: 0.2702\n",
      "Saved and logged samples for epoch 155\n",
      "Epoch 156: Train Loss: 0.0685 | Val Loss: 0.0870\n",
      "Epoch 157: Train Loss: 0.0683 | Val Loss: 0.0447\n",
      "Epoch 158: Train Loss: 0.0690 | Val Loss: 0.0609\n",
      "Epoch 159: Train Loss: 0.0718 | Val Loss: 0.1047\n",
      "Epoch 160: Train Loss: 0.0735 | Val Loss: 0.0700\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep160.png\n",
      "Epoch 160: Val Loss: 0.0700 | CLIP Score: 0.2715\n",
      "Saved and logged samples for epoch 160\n",
      "Epoch 161: Train Loss: 0.0697 | Val Loss: 0.0686\n",
      "Epoch 162: Train Loss: 0.0720 | Val Loss: 0.0865\n",
      "Epoch 163: Train Loss: 0.0682 | Val Loss: 0.0636\n",
      "Epoch 164: Train Loss: 0.0663 | Val Loss: 0.0598\n",
      "Epoch 165: Train Loss: 0.0687 | Val Loss: 0.0689\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep165.png\n",
      "Epoch 165: Val Loss: 0.0689 | CLIP Score: 0.2837\n",
      "Saved and logged samples for epoch 165\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 166: Train Loss: 0.0688 | Val Loss: 0.0623\n",
      "Epoch 167: Train Loss: 0.0710 | Val Loss: 0.0327\n",
      "--- Saved new best Val model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n",
      "Epoch 168: Train Loss: 0.0667 | Val Loss: 0.0776\n",
      "Epoch 169: Train Loss: 0.0707 | Val Loss: 0.0701\n",
      "Epoch 170: Train Loss: 0.0728 | Val Loss: 0.0484\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep170.png\n",
      "Epoch 170: Val Loss: 0.0484 | CLIP Score: 0.2701\n",
      "Saved and logged samples for epoch 170\n",
      "Epoch 171: Train Loss: 0.0682 | Val Loss: 0.0548\n",
      "Epoch 172: Train Loss: 0.0656 | Val Loss: 0.0844\n",
      "Epoch 173: Train Loss: 0.0653 | Val Loss: 0.0779\n",
      "Epoch 174: Train Loss: 0.0688 | Val Loss: 0.0903\n",
      "Epoch 175: Train Loss: 0.0647 | Val Loss: 0.0545\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep175.png\n",
      "Epoch 175: Val Loss: 0.0545 | CLIP Score: 0.2640\n",
      "Saved and logged samples for epoch 175\n",
      "Epoch 176: Train Loss: 0.0730 | Val Loss: 0.0693\n",
      "Epoch 177: Train Loss: 0.0754 | Val Loss: 0.0671\n",
      "Epoch 178: Train Loss: 0.0729 | Val Loss: 0.0970\n",
      "Epoch 179: Train Loss: 0.0722 | Val Loss: 0.0505\n",
      "Epoch 180: Train Loss: 0.0684 | Val Loss: 0.0667\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep180.png\n",
      "Epoch 180: Val Loss: 0.0667 | CLIP Score: 0.2666\n",
      "Saved and logged samples for epoch 180\n",
      "Epoch 181: Train Loss: 0.0652 | Val Loss: 0.0749\n",
      "Epoch 182: Train Loss: 0.0698 | Val Loss: 0.0838\n",
      "Epoch 183: Train Loss: 0.0706 | Val Loss: 0.0759\n",
      "Epoch 184: Train Loss: 0.0711 | Val Loss: 0.0868\n",
      "Epoch 185: Train Loss: 0.0697 | Val Loss: 0.0471\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep185.png\n",
      "Epoch 185: Val Loss: 0.0471 | CLIP Score: 0.2551\n",
      "Saved and logged samples for epoch 185\n",
      "Epoch 186: Train Loss: 0.0648 | Val Loss: 0.0739\n",
      "Epoch 187: Train Loss: 0.0674 | Val Loss: 0.0337\n",
      "Epoch 188: Train Loss: 0.0684 | Val Loss: 0.0512\n",
      "Epoch 189: Train Loss: 0.0637 | Val Loss: 0.0724\n",
      "Epoch 190: Train Loss: 0.0759 | Val Loss: 0.0702\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep190.png\n",
      "Epoch 190: Val Loss: 0.0702 | CLIP Score: 0.2640\n",
      "Saved and logged samples for epoch 190\n",
      "Epoch 191: Train Loss: 0.0704 | Val Loss: 0.0784\n",
      "Epoch 192: Train Loss: 0.0670 | Val Loss: 0.0549\n",
      "Epoch 193: Train Loss: 0.0674 | Val Loss: 0.0557\n",
      "Epoch 194: Train Loss: 0.0705 | Val Loss: 0.0609\n",
      "Epoch 195: Train Loss: 0.0687 | Val Loss: 0.0673\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep195.png\n",
      "Epoch 195: Val Loss: 0.0673 | CLIP Score: 0.2786\n",
      "Saved and logged samples for epoch 195\n",
      "Epoch 196: Train Loss: 0.0677 | Val Loss: 0.0676\n",
      "Epoch 197: Train Loss: 0.0705 | Val Loss: 0.0496\n",
      "Epoch 198: Train Loss: 0.0730 | Val Loss: 0.0735\n",
      "Epoch 199: Train Loss: 0.0665 | Val Loss: 0.0888\n",
      "Saved samples to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/data/05_images/sample_ep199.png\n",
      "Epoch 199: Val Loss: 0.0888 | CLIP Score: 0.2870\n",
      "Saved and logged samples for epoch 199\n",
      "--- Saved new best CLIP model to /home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/checkpoints ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>clip_score</td><td>▁▂▃▁▃▃▃▅▅▅▆▆▇▇▆▇▆▇▆▇▇▇▇▇▇▇▆▇█▇▇▇▇█▇▆▇▆▆█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>epoch_time_sec</td><td>▁▁█▁▁▁▁▁▁▁▁▁█▁▁▁▁█▁█▁▁█▁▁▁█▁▁█▁▁█▁█▁█▁██</td></tr><tr><td>learning_rate</td><td>████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>peak_gpu_mem_mb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▃▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▅▅▅▅▃▃▄▄▃▃▃▄▃▃▄▃▄▃▃▃▃▃▃▃▄▃▄▁▃▃▂▂▂▃▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>clip_score</td><td>0.28703</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>epoch_time_sec</td><td>18.99681</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>peak_gpu_mem_mb</td><td>6310.4668</td></tr><tr><td>train_loss</td><td>0.06653</td></tr><tr><td>val_loss</td><td>0.0888</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ddpm_unet_training</strong> at: <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/dds30dl5' target=\"_blank\">https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2/runs/dds30dl5</a><br> View project at: <a href='https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2' target=\"_blank\">https://wandb.ai/handsoncv-research/diffusion-model-assessment-v2</a><br>Synced 4 W&B file(s), 241 media file(s), 945 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260118_001259-dds30dl5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training Configuration \n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "SUBSET_SIZE = len(train_ds) + len(val_ds) \n",
    "\n",
    "# Initialize Model & & DDPM recommended hyperparameters \n",
    "T = 400\n",
    "IMG_CH = 3\n",
    "IMG_SIZE = train_loader.dataset[0][0].shape[-1]\n",
    "BETAS = torch.linspace(0.0001, 0.02, T).to(DEVICE)\n",
    "# For OpenAI's CLIP, c_embed_dim is stored in model.visual.output_dim\n",
    "CLIP_EMBED_DIM = clip_model.visual.output_dim \n",
    "\n",
    "# Set Seed again for Ensuring Same Model Initialization at Every Run\n",
    "set_seed(SEED)\n",
    "\n",
    "ddpm = DDPM(BETAS, DEVICE)\n",
    "model = UNet(\n",
    "    T, \n",
    "    IMG_CH, \n",
    "    IMG_SIZE, \n",
    "    down_chs=(256, 256, 512), \n",
    "    t_embed_dim=8, \n",
    "    c_embed_dim=CLIP_EMBED_DIM\n",
    ").to(DEVICE)\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    'min', \n",
    "    patience=15, # We wait 15 epochs before cutting LR\n",
    "    factor=0.5,   # We don'tcut it so aggressively\n",
    "    min_lr=5e-5 # We stop the LR from dropping below 5e-5\n",
    ")\n",
    "BOTTLE_EMB_CHANNELS = model.down2.model[-2].model[0].out_channels\n",
    "\n",
    "# Define list of text prompts to generate images for \n",
    "text_list = [\n",
    "    \"A round white daisy with a yellow center\",\n",
    "    \"An orange sunflower with a big brown center\",\n",
    "    \"A deep red rose flower\"\n",
    "]\n",
    "\n",
    "# Initialize W&B Run\n",
    "run = wandb.init(\n",
    "    project=\"diffusion-model-assessment-v2\", \n",
    "    name=\"ddpm_unet_training\",\n",
    "    config={\n",
    "        \"architecture\": \"ddpm_unet\",\n",
    "        \"strategy\": \"generative_modeling_without_ema_without_selfatt_without_aug\",\n",
    "        \"downsample_mode\": \"maxpool\",\n",
    "        \"embedding_size\": BOTTLE_EMB_CHANNELS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"optimizer_type\": \"Adam\",\n",
    "        \"subset_size\": SUBSET_SIZE,\n",
    "        \"seed\": SEED,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Execute Training\n",
    "train_diffusion(\n",
    "    model=model,\n",
    "    ddpm=ddpm,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    drop_prob=0.1,\n",
    "    save_dir=CHECKPOINTS_DIR,\n",
    "    sample_save_dir=SAMPLE_DIR,\n",
    "    clip_model=clip_model,   # Pass the clip model for evaluation\n",
    "    clip_preprocess=clip_preprocess,  # Pass the clip preprocess for evaluation\n",
    "    cond_list=text_list,   # Pass the text prompts list for evaluation\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsoncv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

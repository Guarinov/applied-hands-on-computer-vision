{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da438df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Seeds set to 42 for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import os \n",
    "\n",
    "# On a multi-GPU system, this hides all GPUs except the first \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom modules\n",
    "from handsoncv.datasets import TFflowersCLIPDataset\n",
    "from handsoncv.models import UNet\n",
    "from handsoncv.metrics import extract_inception_features\n",
    "from handsoncv.utils import DDPM, set_seed, seed_worker\n",
    "from handsoncv.evaluation import Evaluator\n",
    "\n",
    "# Hardware & Paths\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\", \"..\"))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# Folders we frequently use across the experiments' notebooks\n",
    "ROOT_PATH = os.path.join(PROJECT_ROOT, \"Assignment-3\")\n",
    "ROOT_DATA = os.path.join(ROOT_PATH, \"data\")\n",
    "DATA_DIR = f\"{ROOT_DATA}/cropped_flowers\"\n",
    "SAMPLE_DIR = f\"{ROOT_DATA}/05_images\"\n",
    "CSV_PATH = f\"{ROOT_DATA}/clip_embeddings_metadata.csv\"\n",
    "\n",
    "CHECKPOINTS_DIR = os.path.join(ROOT_PATH, \"checkpoints\")\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "# Numpy and Torch Reproducibility\n",
    "SEED=42\n",
    "set_seed(42)\n",
    "\n",
    "# Base Configuration Parameters\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043c0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNet/DDPM trained in notebook '05_a_*'\n",
    "model = UNet(400, 3, 32, down_chs=(256, 256, 512)).to(DEVICE)\n",
    "model.load_state_dict(torch.load(f\"{CHECKPOINTS_DIR}/ddpm_unet_best_clip_model.pt\"))\n",
    "ddpm = DDPM(torch.linspace(0.0001, 0.02, 400).to(DEVICE), DEVICE)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d4e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from real images for FID...\n"
     ]
    }
   ],
   "source": [
    "# Create a Generator object to pass to the dataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Base transforms used by both training and validation data\n",
    "base_t = [\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "]\n",
    "\n",
    "# Create a DataLoader for original (real) images\n",
    "ds = TFflowersCLIPDataset(CSV_PATH, transform=transforms.Compose(base_t))\n",
    "data_loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb719890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID Score: 203.0053355017136\n"
     ]
    }
   ],
   "source": [
    "# Assessment Part 1 & 2: Generation, Embedding Extraction, CLIP Score and FID\n",
    "# For inspection of the exact functions, please refer to sample_flowers in src/handsoncv/utils.py and Evaluator class in src/handsoncv/evaluation.py\n",
    "evaluator = Evaluator(model, ddpm, clip_model, clip_preprocess, DEVICE, results_dir=\"results/eval_01\")\n",
    "\n",
    "# Define list of text prompts to generate images for\n",
    "text_prompts = [\n",
    "    \"A red rose flower\",\n",
    "    \"A deep red rose\",\n",
    "    \"A rose with layered petals\",\n",
    "    \"A red rose with layered petals\",\n",
    "    \"A pink rose flower\",\n",
    "    \"A detailed rose flower\",\n",
    "    \"A close-up of a rose\",\n",
    "    \n",
    "    \"Two sunflowers with big brown centers\",\n",
    "    \"A sunflower flower\",\n",
    "    \"A sunflower with bright yellow petals\",\n",
    "    \"An orange sunflower with a big brown center\",\n",
    "    \"A bright yellow sunflower\",\n",
    "    \"A close-up of a sunflower\",\n",
    "    \"A large sunflower\",\n",
    "    \"A sunflower with limp, drooping petals\",\n",
    "    \n",
    "    \"A white daisy with a yellow center\",\n",
    "    \"A round white daisy\",\n",
    "    \"A daisy flower\",\n",
    "    \"A detailed daisy flower\",\n",
    "    \"A close-up of a daisy\",\n",
    "    \"A daisy covered in dew\",\n",
    "    \"Two daisies\",\n",
    "    \"Two white daisies with yellow centers\",\n",
    "]\n",
    "\n",
    "eval_results, fid = evaluator.run_full_evaluation(\n",
    "    text_prompts,\n",
    "    real_features=real_features\n",
    ")\n",
    "\n",
    "print(f\"FID Score: {fid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54681c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'A red rose flower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_000.png',\n",
       "  'clip_score': 0.29248046875,\n",
       "  'embedding': array([ 2.177457  ,  2.7260983 ,  0.52471787, ...,  0.07317976,\n",
       "         -0.16184847,  0.43653056], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A deep red rose',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_001.png',\n",
       "  'clip_score': 0.287841796875,\n",
       "  'embedding': array([ 2.5545046 ,  4.202106  ,  1.551464  , ..., -0.16619082,\n",
       "         -0.15627752,  0.44599095], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A rose with layered petals',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_002.png',\n",
       "  'clip_score': 0.27490234375,\n",
       "  'embedding': array([ 3.6166503 ,  3.7612288 ,  0.37931985, ...,  0.59454894,\n",
       "         -0.15244798, -0.0718812 ], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A red rose with layered petals',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_003.png',\n",
       "  'clip_score': 0.284912109375,\n",
       "  'embedding': array([ 1.2850711 ,  0.9938127 , -0.15909609, ...,  0.20333181,\n",
       "          0.00720037,  0.47369406], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A pink rose flower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_004.png',\n",
       "  'clip_score': 0.2734375,\n",
       "  'embedding': array([ 1.6660476 ,  2.0520148 ,  0.35509834, ..., -0.11750843,\n",
       "         -0.16566713,  0.35898855], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A detailed rose flower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_005.png',\n",
       "  'clip_score': 0.251220703125,\n",
       "  'embedding': array([ 0.49965173,  0.4164385 , -0.06279454, ...,  0.5680458 ,\n",
       "          0.61872196,  0.0447445 ], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A close-up of a rose',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_006.png',\n",
       "  'clip_score': 0.258056640625,\n",
       "  'embedding': array([ 1.0826316 ,  1.2494632 , -0.01293343, ..., -0.15814547,\n",
       "          1.1934363 ,  1.0550672 ], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'Two sunflowers with big brown centers',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_007.png',\n",
       "  'clip_score': 0.278564453125,\n",
       "  'embedding': array([-0.02270659, -0.1012658 , -0.1040241 , ...,  0.6297735 ,\n",
       "          0.50716025,  0.35916853], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A sunflower flower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_008.png',\n",
       "  'clip_score': 0.300537109375,\n",
       "  'embedding': array([-0.07877787, -0.16621362, -0.15597714, ...,  0.33151716,\n",
       "          0.16051129,  0.5729582 ], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A sunflower with bright yellow petals',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_009.png',\n",
       "  'clip_score': 0.30078125,\n",
       "  'embedding': array([-0.16119663, -0.16974042, -0.13731752, ...,  0.5195234 ,\n",
       "          0.29350707,  0.17997667], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'An orange sunflower with a big brown center',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_010.png',\n",
       "  'clip_score': 0.276123046875,\n",
       "  'embedding': array([0.39292464, 0.6582283 , 0.10454153, ..., 0.5834151 , 0.00834519,\n",
       "         0.26980782], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A bright yellow sunflower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_011.png',\n",
       "  'clip_score': 0.313232421875,\n",
       "  'embedding': array([-0.12176686, -0.16259466, -0.14803009, ...,  0.83561516,\n",
       "          0.7077032 ,  0.46170843], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A close-up of a sunflower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_012.png',\n",
       "  'clip_score': 0.2841796875,\n",
       "  'embedding': array([-0.05365788, -0.02296607,  0.03439866, ...,  0.14614636,\n",
       "          0.22309628,  0.21314795], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A large sunflower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_013.png',\n",
       "  'clip_score': 0.3046875,\n",
       "  'embedding': array([ 0.31854877,  0.18824732, -0.09063187, ...,  0.23238194,\n",
       "          0.22251157,  0.21915062], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A sunflower with limp, drooping petals',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_014.png',\n",
       "  'clip_score': 0.28369140625,\n",
       "  'embedding': array([ 0.6073118 ,  0.6478061 , -0.09667601, ...,  0.08300085,\n",
       "          0.07896347,  0.28292233], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A white daisy with a yellow center',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_015.png',\n",
       "  'clip_score': 0.277587890625,\n",
       "  'embedding': array([0.60069686, 0.5754963 , 0.5581088 , ..., 0.95374775, 0.7477533 ,\n",
       "         0.49879488], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A round white daisy',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_016.png',\n",
       "  'clip_score': 0.307373046875,\n",
       "  'embedding': array([1.5854955 , 2.11831   , 0.8139345 , ..., 1.0602778 , 0.22759953,\n",
       "         0.48806578], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A daisy flower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_017.png',\n",
       "  'clip_score': 0.294921875,\n",
       "  'embedding': array([0.9937433, 1.5704547, 0.7479101, ..., 0.791289 , 1.3194169,\n",
       "         0.8030699], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A detailed daisy flower',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_018.png',\n",
       "  'clip_score': 0.292724609375,\n",
       "  'embedding': array([0.5931941 , 0.76459813, 0.15016192, ..., 1.4058081 , 0.11995781,\n",
       "         0.497112  ], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A close-up of a daisy',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_019.png',\n",
       "  'clip_score': 0.30859375,\n",
       "  'embedding': array([0.76538247, 1.0363473 , 0.10209106, ..., 0.7305676 , 0.24729742,\n",
       "         0.444015  ], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'A daisy covered in dew',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_020.png',\n",
       "  'clip_score': 0.260498046875,\n",
       "  'embedding': array([ 1.7601038 ,  2.096067  ,  1.0426996 , ...,  0.83593416,\n",
       "         -0.01605919,  0.71676195], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'Two daisies',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_021.png',\n",
       "  'clip_score': 0.22998046875,\n",
       "  'embedding': array([ 0.05314848, -0.08929674, -0.16485702, ...,  1.2664169 ,\n",
       "          0.51909375,  0.45397186], shape=(32768,), dtype=float32)},\n",
       " {'prompt': 'Two white daisies with yellow centers',\n",
       "  'img_path': '/home/vanessa/Documents/repos/Applied-Hands-On-Computer-Vision/Assignment-3/notebooks/results/eval_01/gen_022.png',\n",
       "  'clip_score': 0.291748046875,\n",
       "  'embedding': array([ 0.13162343, -0.02296042, -0.00877895, ...,  0.71730614,\n",
       "          0.14683396,  0.743164  ], shape=(32768,), dtype=float32)}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c3057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 23/23 [182.3ms elapsed, 0s remaining, 128.4 samples/s]    \n",
      "Computing brain metrics...\n",
      "Computing embeddings...\n",
      " 100% |███████████████████| 23/23 [1.3s elapsed, 0s remaining, 17.9 samples/s]      \n",
      "Computing uniqueness...\n",
      "Uniqueness computation complete\n",
      "Computing representativeness...\n",
      "Computing clusters for 23 embeddings; this may take awhile...\n",
      "Representativeness computation complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=f139bdaf-5fd6-4d06-9944-06acb42229d1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7309ef910050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assessment Part 3: FiftyOne Analysis\n",
    "dataset = fo.Dataset(name=\"generated_flowers_eval\", overwrite=True)\n",
    "samples = []\n",
    "\n",
    "# eval_results now contains 21 items (3 prompts * 7 guidance scales)\n",
    "for res in eval_results:\n",
    "    sample = fo.Sample(filepath=res[\"img_path\"])\n",
    "    sample[\"prompt\"] = fo.Classification(label=res[\"prompt\"])\n",
    "    sample[\"clip_score\"] = res[\"clip_score\"]\n",
    "    sample[\"unet_embedding\"] = res[\"embedding\"]\n",
    "    samples.append(sample)\n",
    "\n",
    "dataset.add_samples(samples)\n",
    "\n",
    "# Run if we have enough samples to satisfy FiftyOne's default clustering\n",
    "if len(dataset) >= 20:\n",
    "    print(\"Computing brain metrics...\")\n",
    "    fob.compute_uniqueness(dataset)\n",
    "    fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")\n",
    "else:\n",
    "    print(f\"Dataset size ({len(dataset)}) is too small for representativeness (needs 20+).\")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832f216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsoncv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
